{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Repo Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1127\n",
      "577\n"
     ]
    }
   ],
   "source": [
    "# TODO: Discuss idea with Professor\n",
    "# Q. What high prompt-density repositories should we be looking at? And, for which parsers?\n",
    "\n",
    "# USING DEFAULT PARSER for now\n",
    "import json\n",
    "with open('../parse_data/repo_to_prompts.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "all_prompts = []\n",
    "high_density_repo_prompts = []\n",
    "for repo in data:\n",
    "    all_prompts.extend(data[repo])\n",
    "    if len(data[repo]) > 10:\n",
    "        high_density_repo_prompts.extend(data[repo])\n",
    "\n",
    "print(len(all_prompts))\n",
    "print(len(high_density_repo_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(high_density_repo_prompts)\n",
    "df.columns = ['prompts']\n",
    "\n",
    "df.to_csv('high_density_repo_prompts.csv', index=False)\n",
    "\n",
    "count = 0\n",
    "for prompt in high_density_repo_prompts:\n",
    "    if \"assistant\" in prompt.lower() or \"assistent\" in prompt.lower():\n",
    "        count += 1\n",
    "        # print(prompt)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❌ **Spell Check** (FAILED) (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('金额数', 19), ('销售方式', 19), ('是否含申购费', 19), ('限额项目', 19), ('单位', 19), ('sk', 15), ('gps', 15), ('severability', 15), ('wilbert', 15), ('alzheimer', 15), ('unenforceability', 15), ('arpa', 15), ('invalidity', 15), ('h', 15), ('startswith', 14), ('syntactically', 14), ('argilla', 13), ('huggingface', 12), ('argillatrainer', 12), ('trainingtask', 12), ('feedbackdataset', 12), ('元', 12), ('含', 12), ('standalone', 12), ('v', 12), ('sqlresult', 12), ('sqlquery', 12), ('llm', 11), ('openapi', 11), ('delimited', 11), ('func', 10), ('demogpt', 10), ('g', 10), ('integrations', 10), ('ux', 10), ('str', 9), ('dict', 9), ('config', 9), ('py', 9), ('ros', 9), ('pl', 9), ('n', 9), ('params', 9), ('coreference', 8), ('db', 7), ('https', 7), ('streamlitcallbackhandler', 6), ('llmmathchain', 6), ('duckduckgosearchrun', 6), ('verbose', 6), ('llmchain', 6), ('申购最低额', 6), ('追加申购最低额', 6), ('份', 6), ('x', 6), ('endpoints', 6), ('attr', 6), ('comparators', 6), ('comparator', 6), ('msgs', 5), ('conversationbuffermemory', 5), ('streamlitchatmessagehistory', 5), ('ui', 5), ('backticks', 5), ('fiftyone', 5), ('agentexecutor', 4), ('cb', 4), ('shutil', 4), ('summarization', 4), ('prompttemplate', 4), ('tuple', 4), ('annotation', 4), ('tuples', 4), ('tensors', 4), ('tokenizer', 4), ('pt', 4), ('trl', 4), ('autotokenizer', 4), ('pretrained', 4), ('sshleifer', 4), ('filename', 4), ('modal', 4), ('直销中心柜台', 4), ('网上直销系统', 4), ('其他销售机构', 4), ('システム', 4), ('上記の資料に基づいて以下の質問について資料から抜粋して回答を生成します', 4), ('と答えます', 4), ('ユーザー', 4), ('わかりません', 4), ('資料にない内容には答えず', 4), ('正直に', 4), ('システムは資料から抜粋して質問に答えます', 4), ('フォローアップの質問', 4), ('フォローアップの質問を独立した質問に言い換えてください', 4), ('独立した質問', 4), ('次のような会話とフォローアップの質問に基づいて', 4), ('serpapi', 4), ('km', 4), ('alava', 4), ('sparql', 4), ('元的', 4), ('您是一位专业的鲜花店文案撰写员', 4), ('对于售价为', 4), ('weibo', 4), ('微博', 4), ('tempfile', 3), ('conversationalchatagent', 3), ('agenttype', 3), ('validations', 3), ('hashtags', 3), ('summarizer', 3), ('humanmessageprompttemplate', 3), ('chatprompttemplate', 3), ('systemmessageprompttemplate', 3), ('kwargs', 3), ('c', 3), ('eos', 3), ('parsed', 3), ('请说', 3), ('问题', 3), ('dependencies', 3), ('qubits', 3), ('qubit', 3), ('xml', 3), ('img', 3), ('semantically', 3), ('whitespace', 3), ('documentated', 3), ('eq', 3), ('lt', 3), ('transcirpt', 3), ('factuality', 3), ('inacurracies', 3), ('您能提供一个吸引人的简短描述吗', 3), ('namedtemporaryfile', 2), ('llms', 2), ('isdir', 2), ('rmtree', 2), ('chatbot', 2), ('convertdoctostring', 2), ('writeblogpost', 2), ('releated', 2), ('minilm', 2), ('epochs', 2), ('gpu', 2), ('automodelforcausallm', 2), ('禁止胡乱编造', 2), ('简要的回答用户的问题', 2), ('基于以下已知的信息', 2), ('专业', 2), ('知识库中提供的内容不足以回答此问题', 2), ('已知内容', 2), ('如果无法从提供的内容中获取答案', 2), ('txt', 2), ('png', 2), ('xxx', 2), ('电子直销交易系统', 2), ('赎回最低额', 2), ('账户持有份额下限', 2), ('销售机构', 2), ('直销中心', 2), ('swahili', 2), ('www', 2), ('nyc', 2), ('opentable', 2), ('pm', 2), ('typesubmit', 2), ('redfin', 2), ('newell', 2), ('chatgpt', 2), ('stg', 2), ('foaf', 2), ('http', 2), ('xmlns', 2), ('prefixes', 2), ('curdate', 2), ('elasticsearch', 2), ('esquery', 2), ('gt', 2), ('detections', 2), ('pred', 2), ('mistakenness', 2), ('datasetview', 2), ('花评人对上述花的评论', 2), ('鲜花介绍', 2), ('永远不要忘记我们的任务', 2), ('我是', 2), ('这是任务', 2), ('不要添加任何其他内容', 2), ('道德', 2), ('如果由于物理', 2), ('永远不要忘记你是', 2), ('写一个简单的总结', 2), ('找一些他比较感兴趣的事情', 2), ('请你帮我', 2), ('写一篇热情洋溢的介绍信', 2), ('下面是这个人的微博信息', 2), ('下面是需要你来回答的问题', 2), ('jmps', 1), ('brwn', 1), ('lze', 1), ('streamlit', 1), ('getenv', 1), ('predefined', 1), ('args', 1), ('solvemathproblem', 1), ('summarizedoc', 1), ('formattable', 1), ('setfit', 1), ('peft', 1), ('sm', 1), ('uuid', 1), ('ft', 1), ('chatcompletion', 1), ('annotator', 1), ('sft', 1), ('iterator', 1), ('generationconfig', 1), ('logits', 1), ('rm', 1), ('truncation', 1), ('automodelforsequenceclassification', 1), ('tokenize', 1), ('ppo', 1), ('dpo', 1), ('下面是一份示例数据', 1), ('请一步一步思考', 1), ('请以json格式返回您的答案', 1), ('分析各列数据的含义和作用', 1), ('请学习理解该数据的结构和内容', 1), ('并对专业术语进行简单明了的解释', 1), ('提供一些分析方案思路', 1), ('返回格式如下', 1), ('最多返回', 1), ('并且能被', 1), ('格式如下', 1), ('已知表结构信息如下', 1), ('库解析', 1), ('格式回答', 1), ('返回多少条数据', 1), ('使用', 1), ('注意', 1), ('注意哪一列位于哪张表中', 1), ('禁止随意捏造信息', 1), ('如果用户没有在问题中指定', 1), ('语句', 1), ('语言的', 1), ('只能使用表结构信息中提供的表来生成', 1), ('你会生成一条对应的', 1), ('那么你生成的', 1), ('语法的', 1), ('不要查询不存在的列', 1), ('提供的表结构信息不足以生成', 1), ('你是一个', 1), ('给你一个用户的问题', 1), ('你应该尽可能少地使用表', 1), ('格式', 1), ('如果无法根据提供的表结构中生成', 1), ('查询', 1), ('专家', 1), ('确保你的回答是必须是正确的', 1), ('条数据', 1), ('点进行总结', 1), ('请根据提供的上下文信息的进行总结', 1), ('libs', 1), ('granularity', 1), ('frontend', 1), ('init', 1), ('prd', 1), ('apis', 1), ('utils', 1), ('seperatedly', 1), ('bcrypt', 1), ('bulletpoint', 1), ('tl', 1), ('dr', 1), ('bulletpoints', 1), ('并提供连贯且与手头主题相关的响应', 1), ('但它有一系列工具来完成不同的视觉任务', 1), ('有些工具将会返回英文描述', 1), ('会使用其他视觉问答工具或描述工具来观察真实图像', 1), ('在使用工具生成新的图像文件时', 1), ('理解这个图像', 1), ('agent也知道图像可能与用户需求不一样', 1), ('但', 1), ('从回答简单的问题到提供对广泛主题的深入解释和讨论', 1), ('描述帮助', 1), ('并提供关于范围广泛的主题的有价值的见解和信息', 1), ('使其能够进行听起来自然的对话', 1), ('如果生成新图像', 1), ('不能直接读取图像', 1), ('格式为', 1), ('总的来说', 1), ('可以使用这些工具', 1), ('而不是伪造图像内容和图像文件名', 1), ('对文件名的要求非常严格', 1), ('它将记得提供上次工具观察的文件名', 1), ('提供带有描述的新图形', 1), ('能够处理和理解大量文本和图像', 1), ('agent可以调用不同的工具来间接理解图片', 1), ('并且忠于工具观察输出', 1), ('作为一种语言模型', 1), ('能够根据收到的输入生成类似人类的文本', 1), ('绝不会伪造不存在的文件', 1), ('应该使用工具来完成以下任务', 1), ('能够按顺序使用工具', 1), ('但你对用户的聊天应当采用中文', 1), ('是一个强大的可视化对话辅助工具', 1), ('在谈论图片时', 1), ('而不是直接从描述中想象', 1), ('工具列表', 1), ('旨在能够协助完成范围广泛的文本和视觉相关任务', 1), ('每张图片都会有一个文件名', 1), ('可以帮助处理范围广泛的任务', 1), ('可能会向', 1), ('用户使用中文和你进行聊天', 1), ('如果要调用工具', 1), ('但是工具的参数应当使用英文', 1), ('你必须遵循如下格式', 1), ('而是对观察结果进行总结回复时', 1), ('你必须使用如下格式', 1), ('当你不再需要继续调用工具', 1), ('工具的参数只能是英文', 1), ('agent是一个文本语言模型', 1), ('而且永远不会伪造不存在的文件', 1), ('新输入', 1), ('开始', 1), ('必须使用工具去观察图片而不是依靠想象', 1), ('推理想法和观察结果只对worker', 1), ('你只能给用户返回中文句子', 1), ('在你使用工具时', 1), ('我们一步一步思考', 1), ('聊天历史', 1), ('你对文件名的正确性非常严格', 1), ('agent可见', 1), ('需要记得在最终回复时把重要的信息重复给用户', 1), ('因为worker', 1), ('contextually', 1), ('sops', 1), ('automating', 1), ('proactively', 1), ('centric', 1), ('placeholders', 1), ('catkin', 1), ('cmakelists', 1), ('cmake', 1), ('maintainers', 1), ('pytest', 1), ('setuptools', 1), ('ament', 1), ('cfg', 1), ('最小申购赎回单位', 1), ('转换最低额', 1), ('followup', 1), ('newmark', 1), ('customcomponent', 1), ('baseretriever', 1), ('basememory', 1), ('basellm', 1), ('baseoutputparser', 1), ('baseloader', 1), ('langflow', 1), ('textsplitter', 1), ('nesteddict', 1), ('vectorstore', 1), ('embeddings', 1), ('basechatmemory', 1), ('midjourney', 1), ('numexpr', 1), ('succint', 1), ('inluding', 1), ('recomendation', 1), ('subgoal', 1), ('pc', 1), ('dbt', 1), ('rdeb', 1), ('ancsa', 1), ('exerpts', 1), ('comversation', 1), ('americorps', 1), ('cobol', 1), ('withing', 1), ('subprograms', 1), ('nebulagraph', 1), ('p', 1), ('kùzu', 1), ('whitespaces', 1), ('mbox', 1), ('cratedb', 1), ('duckdb', 1), ('googlesql', 1), ('ms', 1), ('getdate', 1), ('mysql', 1), ('mariadb', 1), ('sysdate', 1), ('trunc', 1), ('postgresql', 1), ('sqlite', 1), ('clic', 1), ('clickhouse', 1), ('prestodb', 1), ('hmm', 1), ('readthedocs', 1), ('hyperlinks', 1), ('hyperlink', 1), ('将后续输入问题改写为独立问题', 1), ('后续输入问题', 1), ('独立问题', 1), ('聊天记录', 1), ('给定以下聊天记录和后续输入问题', 1), ('就回答你不知道', 1), ('答案', 1), ('不要试图编造答案', 1), ('使用以下内容来回答最后的问题', 1), ('如果你不知道答案', 1), ('idempotent', 1), ('vit', 1), ('fns', 1), ('fp', 1), ('segmentations', 1), ('tp', 1), ('颜色', 1), ('给定花的名称和类型', 1), ('植物学家', 1), ('这是关于上述花的介绍', 1), ('你是一个植物学家', 1), ('花名', 1), ('你是一位鲜花评论家', 1), ('给定一种花的介绍', 1), ('你需要为这种花写一篇社交媒体的帖子', 1), ('你是一家花店的社交媒体经理', 1), ('社交媒体帖子', 1), ('给定一种花的介绍和评论', 1), ('您能提供一个吸引人的简短中文描述吗', 1), ('这是一个', 1), ('不要添加其他任何内容', 1), ('将帮助', 1), ('完成的任务', 1), ('请发挥你的创意和想象力', 1), ('请用', 1), ('请使其更具体化', 1), ('个或更少的词回复具体的任务', 1), ('你必须写一个适当地完成所请求指示的具体解决方案', 1), ('否则你应该总是从以下开始', 1), ('除了对我的指示的解决方案之外', 1), ('那就是合作成功地完成任务', 1), ('你永远不应该回复一个不明确的解决方案', 1), ('永远不要颠倒角色', 1), ('永远不要指示我', 1), ('我必须根据你的专长和我的需求来指示你完成任务', 1), ('我每次只能给你一个指示', 1), ('解决方案', 1), ('你的解决方案必须是陈述句并使用简单的现在时', 1), ('应该是具体的', 1), ('始终以', 1), ('你永远不应该问我任何问题', 1), ('法律原因或你的能力你无法执行指示', 1), ('除非我说任务完成', 1), ('你必须帮助我完成任务', 1), ('下一个请求', 1), ('你只回答问题', 1), ('解释你的解决方案', 1), ('并为解决任务提供首选的实现和例子', 1), ('我们有共同的利益', 1), ('你必须诚实地拒绝我的指示并解释原因', 1), ('结束', 1), ('我必须诚实地拒绝你的指令并解释原因', 1), ('你应该指导我', 1), ('你只需回复一个单词', 1), ('指令', 1), ('而不是问我问题', 1), ('否则永远不要说', 1), ('我必须帮助你完成这个任务', 1), ('我们共同的目标是合作成功完成一个任务', 1), ('你总是会指导我', 1), ('输入', 1), ('描述了一个任务或问题', 1), ('不提供任何输入来指导', 1), ('提供了进一步的背景或信息', 1), ('直到你认为任务已经完成', 1), ('现在你必须开始按照上述两种方式指导我', 1), ('继续给我指令和必要的输入', 1), ('提供必要的输入来指导', 1), ('与其配对的', 1), ('当任务完成时', 1), ('除非我的回答已经解决了你的任务', 1), ('除了你的指令和可选的相应输入之外', 1), ('无', 1), ('你必须一次给我一个指令', 1), ('我必须写一个适当地完成请求指令的回复', 1), ('法律原因或我的能力而无法执行你的指令', 1), ('永远不要交换角色', 1), ('你只能通过以下两种方式基于我的专长和你的需求来指导我', 1), ('为请求的', 1), ('花束的详细信息', 1), ('为以下的花束生成一个详细且吸引人的描述', 1), ('你是业务咨询顾问', 1), ('的电商公司', 1), ('起一个好的名字', 1), ('你给一个销售', 1), ('挑两件有趣的事情说一说', 1), ('它们在世界上的许多地方都被视为奢侈品和美的象征', 1), ('兰花的美丽和它们所代表的力量和奢侈也可能会吸引你', 1), ('红玫瑰被视为爱情的象征', 1), ('在许多文化中', 1), ('同时也可以传达出强烈的感情', 1), ('我想要一些独特和奇特的花', 1), ('我会按部就班的思考', 1), ('我也会向客户解释我这样推荐的原因', 1), ('作为一个为花店电商公司工作的ai助手', 1), ('人类', 1), ('给出我的推荐', 1), ('这是因为它们的红色通常与热情和浓烈的感情联系在一起', 1), ('选择兰花可以满足你对独特和奇特的要求', 1), ('我想找一种象征爱情的花', 1), ('红玫瑰不仅能够象征爱情', 1), ('而且', 1), ('我会推荐红玫瑰', 1), ('因此', 1), ('从你的需求中', 1), ('示例', 1), ('我建议你考虑兰花', 1), ('兰花是一种非常独特并且颜色鲜艳的花', 1), ('最后根据这个需求', 1), ('然后考虑各种鲜花的涵义', 1), ('同时', 1), ('我理解你正在寻找一种可以象征爱情的花', 1), ('我的目标是帮助客户根据他们的喜好做出明智的决定', 1), ('我理解你想要的是独一无二和引人注目的花朵', 1), ('首先', 1), ('考虑到这一点', 1), ('先理解客户的需求', 1), ('这是你在寻找的', 1), ('assitiant', 1), ('挑两件有趣的特点说一说', 1), ('擅长解答关于养花育花的问题', 1), ('你是一个经验丰富的园丁', 1), ('擅长解答关于鲜花装饰的问题', 1), ('你是一位网红插花大师', 1), ('singleplayer', 1), ('prioritising', 1), ('reworded', 1), ('summarised', 1), ('artstation', 1)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "from spacy.language import Language\n",
    "import re\n",
    "\n",
    "misspelled_freq = {}\n",
    "@Language.component(\"spellcheck_component\")\n",
    "def spellcheck_component(doc):\n",
    "    # Initialize the SpellChecker\n",
    "    spell_checker = SpellChecker()\n",
    "    spell_checker.word_frequency.load_words(\n",
    "        [\"f\", \"langchain\", \"openai\", \"markdown\", \"json\", \"api\", \"endpoint\", \"covid\", \"docstore\", \"chatopenai\", \n",
    "         \"append\", \"schema\", \"gpt\", \"\", \" \", \"yyyy\", \"mm\", \"dd\", \"sql\", \"st\", \"dataset\", \"Zelenskyy\"] + \n",
    "        #  These are things picked up by spaCy due to the use of contractions\n",
    "        [\"s\", \"b\", \"d\", \"m\", \"t\"] +\n",
    "        []\n",
    "    )\n",
    "\n",
    "    misspelled = spell_checker.unknown([token.text for token in doc if token.is_alpha])\n",
    "\n",
    "    # Go through the tokens and suggest corrections for misspelled words (idk if this is necessary)\n",
    "    # for word in misspelled:\n",
    "    #     suggestions = spell_checker.candidates(word)\n",
    "    #     if suggestions:\n",
    "    #         print(f\"Original: {word}\")\n",
    "    #         print(f\"Suggestions: {suggestions}\")\n",
    "    \n",
    "    for word in misspelled:\n",
    "        misspelled_freq[word] = misspelled_freq.get(word, 0) + 1\n",
    "\n",
    "    # Return the doc following spaCy's pipeline component requirements\n",
    "    return doc\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add spell-checking component to the pipeline\n",
    "nlp.add_pipe(\"spellcheck_component\")\n",
    "\n",
    "text = \"The quick brwn fox jmps over the lze dog.\"\n",
    "\n",
    "# Process the text with spaCy and check spelling\n",
    "nlp(text)\n",
    "\n",
    "for prompt in high_density_repo_prompts:\n",
    "    modified_string = re.sub(r'((\\{+)(\\s*)(.*?)(\\s*)(\\}+))', ' ', prompt)  # remove {variable}\n",
    "    modified_string = re.sub(r'\\\\n', ' ', modified_string)  # to avoid \"\\nTonight\"\n",
    "    modified_string = re.sub(r\"[^\\w']+\", \" \", modified_string)  # to avoid \"good-morning\" but words like keep \"don't\"\n",
    "    modified_string = re.sub(r\"_+\", \" \", modified_string)  # to avoid \"______\" or \"good__morning\"\n",
    "    doc = nlp(modified_string)\n",
    "\n",
    "print(sorted(misspelled_freq.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧩 **Searching Patterns** (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14194311245358984270 GPPT-3 70 71 assistant\n",
      "14194311245358984270 GPPT-3 84 85 assistant\n",
      "14194311245358984270 GPPT-3 73 74 assistant\n",
      "14194311245358984270 GPPT-3 87 88 assistant\n",
      "14194311245358984270 GPPT-3 8 9 assistant\n",
      "14194311245358984270 GPPT-3 17 18 assistant\n",
      "14194311245358984270 GPPT-3 30 31 assistant\n",
      "14194311245358984270 GPPT-3 113 114 assistant\n",
      "14194311245358984270 GPPT-3 264 265 assistant\n",
      "14194311245358984270 GPPT-3 13 14 assistant\n",
      "14194311245358984270 GPPT-3 25 26 assistant\n",
      "14194311245358984270 GPPT-3 359 360 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 27 28 Assistant\n",
      "14194311245358984270 GPPT-3 5 6 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 6 7 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 60 61 Assistant\n",
      "14194311245358984270 GPPT-3 74 75 assistant\n",
      "14194311245358984270 GPPT-3 133 134 Assistant\n",
      "14194311245358984270 GPPT-3 59 60 Assistant\n",
      "14194311245358984270 GPPT-3 73 74 assistant\n",
      "14194311245358984270 GPPT-3 132 133 Assistant\n",
      "14194311245358984270 GPPT-3 49 50 Assistant\n",
      "14194311245358984270 GPPT-3 60 61 Assistant\n",
      "14194311245358984270 GPPT-3 74 75 assistant\n",
      "14194311245358984270 GPPT-3 133 134 Assistant\n",
      "14194311245358984270 GPPT-3 59 60 Assistant\n",
      "14194311245358984270 GPPT-3 73 74 assistant\n",
      "14194311245358984270 GPPT-3 132 133 Assistant\n",
      "14194311245358984270 GPPT-3 46 47 Assistant\n",
      "14194311245358984270 GPPT-3 60 61 Assistant\n",
      "14194311245358984270 GPPT-3 74 75 assistant\n",
      "14194311245358984270 GPPT-3 133 134 Assistant\n",
      "14194311245358984270 GPPT-3 12 13 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 6 7 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 6 7 assistant\n",
      "14194311245358984270 GPPT-3 23 24 assistant\n",
      "14194311245358984270 GPPT-3 22 23 assistant\n",
      "14194311245358984270 GPPT-3 87 88 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 8 9 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 6 7 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 6 7 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 7 8 assistant\n",
      "14194311245358984270 GPPT-3 12 13 assistant\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "patterns = [\n",
    "    [{\"LOWER\": \"assistant\"}],\n",
    "]\n",
    "\n",
    "matcher.add(\"GPPT-3\", patterns)\n",
    "\n",
    "def find_pattern(text, patterns):\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        print(match_id, string_id, start, end, span.text)\n",
    "\n",
    "    return matches\n",
    "\n",
    "count = 0\n",
    "for p in high_density_repo_prompts:\n",
    "    if find_pattern(p, patterns):\n",
    "        count += 1\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
