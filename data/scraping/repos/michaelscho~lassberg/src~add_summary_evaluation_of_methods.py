"""
This script evaluates different text summarization methods on a dataset of 10 letters.

Methods evaluated:
1. Local summarization using the Hugging Face Transformers library (e.g., T5, BART, Pegasus)
2. Summarization using OpenAI GPT-3 API

The script performs the following steps:
1. Load the dataset of 10 letters
2. Summarize each letter using each summarization method
3. Evaluate the quality of the summaries generated by each method
4. Create a Markdown file with the description of each method and its evaluation outcome

Usage:
1. Install the required Python packages: `pip install transformers openai`
2. Set the OpenAI API key as an environment variable: `export OPENAI_API_KEY='your_api_key_here'`
3. Run the script: `python add_summary_evaluation_of_methods.py`
4. Check the generated Markdown file for the evaluation results: `evaluation_results.md`

Author: Michael Schonhardt
Date: 01.04.2023
"""

import os
#import openai
import config

#openai.api_key = config.openai_key

# 1. Load the dataset of 10 letters
with open(os.path.join(os.getcwd(), "..", "data", "literature", "test_summary_letters.txt"), "r", encoding="utf-8") as file:
    text = file.read()
    letters = text.split("\n")

for letter in letters:
    print(f"The letter has {len(letter)} characters, making a total of {len(letter)/4} token.")


# 2.1: Use chatgpt 3.5 via openai api (English)

def summarize_letter_gpt_3_5_English(letter_text, max_summary_length=150):
    content = f"Summarize the following correspondence between Joseph von Laßberg and Johan Adam Pupikofer in English in {max_summary_length} words:\n\n{letter_text}\n\nSummary:"
    message = [{"role": "user", "content": content}]
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",  
        messages=message,
        temperature=0.7,
        n=1,
        stop=None,
    )
    summary = response.choices[0].message.content
    return summary

def evaluate_gpt_3_5_English():
    summaries = []

    for letter in letters:
        summary = summarize_letter_gpt_3_5_English(letter)
        print(summary)
        summaries.append(summary)
    text = "\n".join(summaries)

    # Save the summaries to file
    with open(os.path.join(os.getcwd(), "..", "analysis", "gpt3","test_summary_letters_gpt_3_5_English.txt"), "w", encoding="utf-8") as file:
        file.write(text)

# 2.2: Use chatgpt 3.5 via openai api (German)
def summarize_letter_gpt_3_5_German(letter_text, max_summary_length=150):
    content = f"Summarize the following correspondence between Joseph von Laßberg and Johan Adam Pupikofer in German in {max_summary_length} words:\n\n{letter_text}\n\nSummary:"
    message = [{"role": "user", "content": content}]
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",  
        messages=message,
        temperature=0.7,
        n=1,
        stop=None,
    )
    summary = response.choices[0].message.content
    return summary

def evaluate_gpt_3_5_German():

    summaries = []

    for letter in letters:
        summary = summarize_letter_gpt_3_5_German(letter)
        print(summary)
        summaries.append(summary)
    text = "\n".join(summaries)

    # Save the summaries to file
    with open(os.path.join(os.getcwd(), "..", "analysis", "gpt3", "test_summary_letters_gpt_3_5_German.txt"), "w", encoding="utf-8") as file:
        file.write(text)
    
# Use chatgpt 4 via openai api
# Done using chatGPT given lack of api access

# Hugging Face GPT-2
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def gpt2_summarize(text, model_name='gpt2', max_length=150):
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)

    inputs = tokenizer.encode("Summarize the following correspondence between Joseph von Laßberg and Johan Adam Pupikofer English in 150 words: " + text, return_tensors="pt", max_length=512, truncation=True)
    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, early_stopping=True, no_repeat_ngram_size=3)

    summary = tokenizer.decode(outputs[0])
    return summary.strip()
def evaluate_gpt2():
    summaries = []

    for letter in letters:
        summary = gpt2_summarize(letter)
        print(summary)
        summaries.append(summary)
    text = "\n".join(summaries)

    # Save the summaries to file
    with open(os.path.join(os.getcwd(), "..", "analysis", "gpt2","test_summary_letters_gpt_2_English.txt"), "w", encoding="utf-8") as file:
        file.write(text)



# Hugging Face T5.
from transformers import T5ForConditionalGeneration, T5Tokenizer

def t5_summarize(text, model_name='t5-base', max_length=450):
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)

    inputs = tokenizer.encode(text, return_tensors="pt", max_length=512, truncation=True)
    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, early_stopping=True)

    summary = tokenizer.decode(outputs[0])
    return summary.strip()

def evaluate_t5():
    summaries = []

    for letter in letters:
        summary = t5_summarize(letter)
        print(summary)
        summaries.append(summary)
    text = "\n".join(summaries)

    # Save the summaries to file
    with open(os.path.join(os.getcwd(), "..", "analysis", "t5","test_summary_letters_t5_English.txt"), "w", encoding="utf-8") as file:
        file.write(text)


# Hugging Face Bart

from transformers import BartForConditionalGeneration, BartTokenizer

def bart_summarize(text, model_name='facebook/bart-large-cnn', max_length=450):
    tokenizer = BartTokenizer.from_pretrained(model_name)
    model = BartForConditionalGeneration.from_pretrained(model_name)

    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
    outputs = model.generate(inputs['input_ids'], max_length=max_length, num_return_sequences=1, early_stopping=True)

    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary.strip()

def evaluate_bart():
    summaries = []

    for letter in letters:
        summary = bart_summarize(letter)
        print(summary)
        summaries.append(summary)
    text = "\n".join(summaries)

    # Save the summaries to file
    with open(os.path.join(os.getcwd(), "..", "analysis", "bart","test_summary_letters_bart_English.txt"), "w", encoding="utf-8") as file:
        file.write(text)


# Hugging Face Pegasus (German)
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
# GermanT5/t5-efficient-gc4-german-base-nl36 Einmalumdiewelt/PegasusXSUM_GNAD
def pegasus_summarize(text: str, model_name="GermanT5/t5-efficient-gc4-german-base-nl36"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    summarizer = pipeline("summarization", model=model, tokenizer=tokenizer)

    summary = summarizer(text, max_length=150, min_length=40, do_sample=False)
    return summary[0]['summary_text']

def evaluate_pegasus():
    summaries = []

    for letter in letters:
        summary = pegasus_summarize(letter)
        print(summary)
        summaries.append(summary)
    text = "\n".join(summaries)

    # Save the summaries to file
    with open(os.path.join(os.getcwd(), "..", "analysis", "pegasus","test_summary_letters_pegasus_English.txt"), "w", encoding="utf-8") as file:
        file.write(text)


# Hugging Face BloomZ
from transformers import AutoTokenizer, AutoModelForCausalLM

def bloomz_summarize(text, model_name='bigscience/bloomz', max_length=150):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)

    inputs = tokenizer.encode("Summarize the following correspondence between Joseph von Laßberg and Johan Adam Pupikofer English in 150 words: " + text, return_tensors="pt", max_length=512, truncation=True)
    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, early_stopping=True, no_repeat_ngram_size=3)

    summary = tokenizer.decode(outputs[0])
    return summary.strip()
def evaluate_bloomz():
    summaries = []

    for letter in letters:
        summary = bloomz_summarize(letter)
        print(summary)
        summaries.append(summary)
    text = "\n".join(summaries)

    # Save the summaries to file
    with open(os.path.join(os.getcwd(), "..", "analysis", "bloomz","test_summary_letters_bloomz_English.txt"), "w", encoding="utf-8") as file:
        file.write(text)





# Evaluate the models

#evaluate_gpt_3_5_English()
#evaluate_gpt_3_5_German()
#evaluate_gpt2()
#evaluate_t5()
#evaluate_bart()
#evaluate_pegasus()
#evaluate_bloomz()