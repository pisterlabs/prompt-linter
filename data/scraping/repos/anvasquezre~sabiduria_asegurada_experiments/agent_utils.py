
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.chat_models import ChatOpenAI
from langchain.agents import Tool, AgentOutputParser, AgentExecutor,LLMSingleActionAgent,AgentOutputParser
from langchain.prompts import StringPromptTemplate
from langchain import LLMChain
from typing import List, Optional, Union, Tuple
from langchain.schema import AgentAction, AgentFinish, OutputParserException
import re

import config
from langchain.callbacks.streaming_stdout_final_only import (
    FinalStreamingStdOutCallbackHandler,
)
from data_utils import connect_db, aconnect_db
import text_templates
import chainlit as cl
from bs4 import BeautifulSoup
import requests

class CustomOutputParser(AgentOutputParser):
    """
    Custom output parser for processing language model outputs and converting them into agent actions or finish signals.

    This class extends the base `AgentOutputParser` class and provides a custom implementation for the `parse` method.

    Args:
        AgentOutputParser (class): The parent class for output parsing.

    Methods:
        parse(llm_output: str) -> Union[AgentAction, AgentFinish]:
            Parses the language model output to extract agent actions or finish signals.

    """
    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:
        """
        Parses the given language model output and returns either an agent action or a finish signal.

        Args:
            llm_output (str): The output generated by the language model.

        Returns:
            Union[AgentAction, AgentFinish]: An instance of `AgentAction` if an action is parsed,
            or an instance of `AgentFinish` if a finish signal is detected.

        Raises:
            OutputParserException: If the output cannot be parsed successfully.

        """
        # Check if agent should finish
        if "Final Answer:" in llm_output:
            return AgentFinish(
                # Return values is generally always a dictionary with a single `output` key
                # It is not recommended to try anything else at the moment :)
                return_values={"output": llm_output.split("Final Answer:")[-1].strip()},
                log=llm_output,
            )
        # Parse out the action and action input
        regex = r"Action\s*\d*\s*:(.*?)\nAction\s*\d*\s*Input\s*\d*\s*:[\s]*(.*)"
        match = re.search(regex, llm_output, re.DOTALL)
        if not match:
            raise OutputParserException(f"Could not parse LLM output: `{llm_output}`")
        action = match.group(1).strip()
        action_input = match.group(2)
        # Return the action and action input
        return AgentAction(tool=action, tool_input=action_input.strip(" ").strip('"'), log=llm_output)

# Set up a prompt template
class CustomPromptTemplate(StringPromptTemplate):
    """
    Custom prompt template for formatting prompts with dynamic information.

    This class extends the base `StringPromptTemplate` class and provides a custom implementation
    for the `format` method to generate prompts with dynamically inserted information.

    Args:
        StringPromptTemplate (class): The parent class for prompt templates.

    Attributes:
        template (str): The template string to be formatted.
        tools (List[Tool]): A list of tools available for use.

    Methods:
        format(**kwargs) -> str:
            Formats the template string with dynamic information and returns the formatted string.

    """
    # The template to use
    template: str
    # The list of tools available
    tools: List[Tool]

    def format(self, **kwargs) -> str:
        """
        Formats the template string with dynamic information and returns the formatted string.

        Args:
            **kwargs: Keyword arguments containing dynamic information for formatting.

        Returns:
            str: The formatted template string.

        """
        # Get the intermediate steps (AgentAction, Observation tuples)
        # Format them in a particular way
        intermediate_steps = kwargs.pop("intermediate_steps")
        thoughts = ""
        for action, observation in intermediate_steps:
            thoughts += action.log
            thoughts += f"\nObservation: {observation}\nThought: "
        # Set the agent_scratchpad variable to that value
        kwargs["agent_scratchpad"] = thoughts
        # Create a tools variable from the list of tools provided
        kwargs["tools"] = "\n".join([f"{tool.name}: {tool.description}" for tool in self.tools])
        # Create a list of tool names for the tools provided
        kwargs["tool_names"] = ", ".join([tool.name for tool in self.tools])
        return self.template.format(**kwargs)


def get_chat_template(
    system_prompt = text_templates.OPEN_AI_TEMPLATE,
                      human_prompt = text_templates.HUMAN_TEMPLATE,
                      ) -> ChatPromptTemplate:
    """
    Generate a chat prompt template for simulating conversational interactions.

    This function constructs a chat prompt template by combining system and human message templates,
    and returns the resulting template for use in generating conversational prompts.

    Args:
        system_prompt (str): The template for the system message prompt. Default is `text_templates.OPEN_AI_TEMPLATE`.
        human_prompt (str): The template for the human message prompt. Default is `text_templates.HUMAN_TEMPLATE`.

    Returns:
        ChatPromptTemplate: A template for generating prompts in conversational interactions.

    """
    
    system_message_prompt = SystemMessagePromptTemplate.from_template(system_prompt)
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_prompt)


    CHAT_PROMPT = ChatPromptTemplate.from_messages(
        [system_message_prompt, human_message_prompt]
    )
    return CHAT_PROMPT

# Set up the base template

def custom_filter_chain(question:str, 
                        db = connect_db(config.COLLECTION_SUMMARY)
                        ) -> Tuple[str,str]:
    """
    Apply a custom filtering chain to retrieve relevant information based on the input question.

    This function applies a custom filtering chain to the input question using a specified database connection,
    and returns the most likely document source and its associated content as a tuple.

    Args:
        question (str): The input question for information retrieval.
        db: The database connection. Default is connected to `config.COLLECTION_SUMMARY`.

    Returns:
        Tuple[str, str]: A tuple containing the most likely document source and its associated content.

    """
    docs = db.similarity_search(question, k = 1)
    most_likely_doc = docs[0].metadata['source']
    content = docs[0].page_content
    return most_likely_doc , content

async def acustom_filter_chain(question:str, 
                        db = aconnect_db(config.COLLECTION_SUMMARY)
                        ) -> Tuple[str,str]:
    """
    Apply an asynchronous custom filtering chain to retrieve relevant information based on the input question.

    This asynchronous function applies a custom filtering chain to the input question using an asynchronous
    database connection, and returns the most likely document source and its associated content as a tuple.

    Args:
        question (str): The input question for information retrieval.
        db: The asynchronous database connection. Default is connected to `config.COLLECTION_SUMMARY`.

    Returns:
        Tuple[str, str]: A tuple containing the most likely document source and its associated content.

    """
    docs = await db.asimilarity_search(question, k = 3)
    most_likely_doc = docs[0].metadata['source']
    content = docs[0].page_content
    return most_likely_doc, content

async def acustom_doc_retrieval(question:str, 
                        db = aconnect_db(config.COLLECTION_SUMMARY),
                        **kwargs
                        ) -> Tuple[List[str], List[str]]:
    """
    Retrieve relevant documents and titles asynchronously based on the input question.

    This asynchronous function retrieves relevant documents and their corresponding titles from the database
    based on the input question, using an asynchronous database connection. It returns a tuple containing
    a list of source documents and a list of document titles.

    Args:
        question (str): The input question for document retrieval.
        db: The asynchronous database connection. Default is connected to `config.COLLECTION_SUMMARY`.

    Returns:
        Tuple[List[str], List[str]]: A tuple containing a list of source documents and a list of document titles.

    """
    if 'k' in kwargs:
        k = kwargs['k']
    else:
        k = 1
    docs = await db.asimilarity_search(question, k = k)
    source_docs = [doc.metadata['source'] for doc in docs]
    title = [doc.page_content for doc in docs]
    return source_docs, title

def get_llm(
    model_name:str= config.OPENAI_MODEL,
    max_tokens:int= config.MAX_TOKENS,
    streaming:bool= True,
    temperature:float=config.TEMPERATURE,
    **kwargs
    ) -> ChatOpenAI:
    """
    Retrieve a ChatOpenAI instance configured for interaction with a language model.

    This function creates and configures a `ChatOpenAI` instance, which is used for interacting with a
    language model provided by OpenAI. The configuration includes parameters such as the model name,
    maximum tokens, streaming behavior, temperature, and any additional keyword arguments.

    Args:
        model_name (str): The name of the language model. Default is configured as `config.OPENAI_MODEL`.
        max_tokens (int): The maximum number of tokens in the generated response. Default is configured as `config.MAX_TOKENS`.
        streaming (bool): Flag indicating whether to use streaming for long conversations. Default is True.
        temperature (float): The temperature parameter affecting the randomness of the generated text. Default is configured as `config.TEMPERATURE`.
        **kwargs: Additional keyword arguments for configuring the ChatOpenAI instance.

    Returns:
        ChatOpenAI: An instance of the ChatOpenAI class configured for interaction with a language model.

    """
    
    llm = ChatOpenAI(temperature=temperature,
                     model_name=model_name, 
                     max_tokens=max_tokens,
                     streaming=streaming, 
                     #callbacks=[FinalStreamingStdOutCallbackHandler()], # Uncomment for streaming output
                     openai_api_key=config.OPENAI_API_KEY,
                     **kwargs) 
    return llm

def custom_qa(question:str, 
              db = connect_db(config.COLLECTION_CHUNKS),
              llm = get_llm(),
              prompt = get_chat_template()
              ) -> dict[str,str,str]:
    """
    Perform a custom question-answering process.

    This function performs a custom question-answering process by filtering relevant documents,
    generating a chat prompt, and obtaining a response from a language model. It returns a dictionary
    containing the response, retrieved documents, and the chat prompt used for generating the response.

    Args:
        question (str): The input question for the question-answering process.
        db: The database connection. Default is connected to `config.COLLECTION_CHUNKS`.
        llm: The ChatOpenAI instance. Default is created using `get_llm()`.
        prompt: The ChatPromptTemplate instance. Default is created using `get_chat_template()`.

    Returns:
        dict[str, str]: A dictionary containing the response, retrieved documents, and chat prompt used.

    """
    response_dict = {}
    likely_doc , content = custom_filter_chain(question)
    docs = db.similarity_search(question, k = 10, filter={"source":likely_doc})
    # Stuffing the content in a single window
    doc_list = [doc.page_content for doc in docs]
    doc_list.append(content)
    chat_prompt_openai= prompt.format_prompt(question=question, context="\n".join(doc_list)).to_messages()
    response = llm(chat_prompt_openai)
    response_dict['response'] = response
    response_dict['docs'] = docs
    response_dict['chat_prompt_openai'] = chat_prompt_openai
    return response_dict


def get_tools(
    ) -> List[Tool]:
    """
    Retrieve a list of tools for use in the PolicyGuru AI system.

    This function constructs and returns a list of Tool instances, each representing a different tool or capability
    available in the PolicyGuru AI system. The tools include functions for greeting, web search, and policy search.

    Returns:
        List[Tool]: A list of Tool instances representing different capabilities of the PolicyGuru AI system.

    """
    def greeting(query):
        """
        Generate a greeting message for the PolicyGuru AI.

        This function generates a greeting message for the PolicyGuru AI system. The greeting includes a placeholder
        for the user's name and informs them about the system's capability to answer questions about policies.

        Args:
            query: query passed to the tool need by default for the Tool definition (not used in the greeting).

        Returns:
            str: A greeting message introducing PolicyGuru AI's capabilities.

        """
        return f"Hola <nombre>, Soy PolicyGuru AI. Puedo responder preguntas sobre polizas de seguro."
    
    def get_news(query):
        """
        Perform a web search for news articles related to the given query.

        This function performs a web search using Google's search engine to find news articles related to the given query.
        It retrieves the titles, snippets, links, dates, and sources of the news articles and compiles them into a formatted text.

        Args:
            query (str): The query for which news articles are searched.

        Returns:
            str: A formatted string containing information about the retrieved news articles.
        """
        # Header required to avoid 403 error
        headers = {
            "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36"
        }
        response = requests.get(
            f"https://www.google.com/search?q={query}&gl=cl&tbm=nws&num=20", headers=headers
        )
        soup = BeautifulSoup(response.content, "html.parser")

        news_results = []
        
        for el in soup.select("div.SoaBEf"):
            news_results.append(
                {
                    "title": el.select_one("div.MBeuO").get_text(),
                    "snippet": el.select_one(".GI74Re").get_text(),
                    "link": el.find("a")["href"],
                    "date": el.select_one(".LfVVr").get_text(),
                    "source": el.select_one(".NUnG9d span").get_text()
                }
            )
        news_string = [f"Titulo\n{news['title']}\nTexto\n{news['snippet']}\nlink\n{news['link']}\nfecha\n{news['date']}\nfuente\n{news['source']}" for news in news_results]
        news_text = "\n\n".join(news_string)
        return news_text
    
    # Directly answer the question
    def qa_tool(question:str) -> str:
        """
        Perform question-answering using a custom question-answering process.

        This function performs a question-answering process using a custom_qa function. It retrieves a response from the
        custom_qa function and returns the content of the response.

        Args:
            question (str): The input question for question-answering.

        Returns:
            str: The content of the response from the question-answering process.

        """
        response_dict = custom_qa(question)
        response = response_dict['response']
        return response.content

    def qa_tool2(question:str,
                 db = connect_db(config.COLLECTION_CHUNKS)
                 ) -> str:
        """
        Performs an augmented retrieval from a database using custom filtering.

        This function performs question-answering using a custom filtering process to retrieve relevant context
        from a database. It first applies a custom filtering chain to obtain the most likely document source and content.
        Then, it retrieves related documents from the database, combines their content with the original content,
        and returns the accumulated context for question-answering.

        Args:
            question (str): The input question for question-answering.
            db: The database connection. Default is connected to `config.COLLECTION_CHUNKS`.

        Returns:
            str: The accumulated context for question-answering, including relevant documents and original content.

        """
        likely_doc, content = custom_filter_chain(question)
        docs = db.similarity_search(question, k = 4, filter={"source":likely_doc})
        docs_list = [doc.page_content for doc in docs]
        docs_list.append(content)
        context="\n".join(docs_list)
        return context
    
    async def aqa_tool2(question:str,
                        db = aconnect_db(config.COLLECTION_CHUNKS)
                        ) -> str:
        """
        Performs an asynchronus (required by Chainlit) augmented retrieval from a database using custom filtering.

        This function performs question-answering using a custom filtering process to retrieve relevant context
        from a database. It first applies a custom filtering chain to obtain the most likely document source and content.
        Then, it retrieves related documents from the database, combines their content with the original content,
        and returns the accumulated context for question-answering.

        Args:
            question (str): The input question for question-answering.
            db: The database connection. Default is connected to `config.COLLECTION_CHUNKS`.

        Returns:
            str: The accumulated context for question-answering, including relevant documents and original content.

        """   
        likely_doc, content = await acustom_filter_chain(question)
        docs = await db.asimilarity_search(question, k = 4, filter={"source":likely_doc})
        docs_list = [doc.page_content for doc in docs]
        docs_list.append(content)
        context=  "\n".join(docs_list)
        return context
    
    tools = [
        Tool(
            name = "Web Search",
            func=get_news,
            description=text_templates.WEB_SEARCH_DESCRIPTION,
            coroutine=cl.make_async(get_news)
        ),
        Tool(
            name = "Policy Search",
            func=qa_tool2,
            description=text_templates.POLICY_SEARCH_DESCRIPTION,
            coroutine=aqa_tool2
        ),
        Tool(
            name = "Greeting",
            func=greeting,
            description=text_templates.GREETING_DESCRIPTION,
            coroutine=cl.make_async(greeting)
        ),
    ]

    return tools

def get_agent_prompt(
    template:str = text_templates.AGENT_TEMPLATE, 
    tools:List[Tool] = get_tools(), 
    input_vars:List[str] = ["input", "intermediate_steps", "history"]
    ) -> CustomPromptTemplate:
    """
    Generate a custom prompt template for interacting with the PolicyGuru AI agent.

    This function generates a custom prompt template for interacting with the PolicyGuru AI agent. The template includes
    placeholders for variables related to input, intermediate steps, and history. The available tools are also included
    dynamically. The generated template can be used to format conversations and interactions with the agent.

    Args:
        template (str): The base template for the agent prompt. Default is `text_templates.AGENT_TEMPLATE`.
        tools (List[Tool]): A list of Tool instances representing available tools for interaction. Default is obtained using `get_tools()`.
        input_vars (List[str]): A list of variable names to include in the prompt template for input, intermediate steps, and history. Default includes ["input", "intermediate_steps", "history"].

    Returns:
        CustomPromptTemplate: A CustomPromptTemplate instance representing the generated prompt template.

    """
    prompt = CustomPromptTemplate(
        template=template,
        tools=tools,
        # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically
        # This includes the `intermediate_steps` variable because that is needed
        input_variables=input_vars,
    )
    return prompt

def create_agent(
    prompt:CustomPromptTemplate = get_agent_prompt(),
    output_parser:CustomOutputParser = CustomOutputParser(),
    llm:ChatOpenAI = get_llm(),
    tools:List[Tool] = get_tools(),
    ) -> AgentExecutor:
    """
    Create an AgentExecutor instance for interacting with the PolicyGuru AI agent.

    This function creates and configures an AgentExecutor instance, which provides an interface for interacting
    with the PolicyGuru AI agent. The agent is powered by a language model (ChatOpenAI), uses a custom prompt template,
    and employs a custom output parser. The available tools are also specified for use in interactions.

    Args:
        prompt (CustomPromptTemplate): The custom prompt template for agent interactions. Default is generated using `get_agent_prompt()`.
        output_parser (CustomOutputParser): The custom output parser for interpreting agent responses. Default is `CustomOutputParser()`.
        llm (ChatOpenAI): The ChatOpenAI instance for generating language model responses. Default is obtained using `get_llm()`.
        tools (List[Tool]): A list of Tool instances representing available tools for interaction. Default is obtained using `get_tools()`.

    Returns:
        AgentExecutor: An AgentExecutor instance configured for interacting with the PolicyGuru AI agent.

    """
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    tool_names = [tool.name for tool in tools]
    agent = LLMSingleActionAgent(
        llm_chain=llm_chain,
        output_parser=output_parser,
        stop=["\nObservation:"],
        allowed_tools=tool_names
    )
    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, 
                                                        tools=tools, 
                                                        verbose=False,
                                                        handle_parsing_errors=True)
    return agent_executor

class ChatBOT():
    """
    A simple chatbot class for interacting with the PolicyGuru AI agent.

    This class represents a chatbot that interacts with the PolicyGuru AI agent to provide responses to user queries.
    It maintains a chat history and allows users to initiate conversations and retrieve related documents.

    Attributes:
        chat_history (List[str]): A list storing the history of user interactions.
        answer (str): The current answer provided by the agent.
        db_query (str): The query used for retrieving related documents.
        db_response: The response containing related documents obtained from the database.
        agent (AgentExecutor): The AgentExecutor instance for interacting with the PolicyGuru AI agent.

    Methods:
        __init__: Initialize the chatbot by creating the PolicyGuru AI agent.
        get_related_docs: Retrieve related documents using custom filtering.
        aget_related_docs: Asynchronously retrieve related documents using custom filtering.
        clr_history: Clear the chat history.
        clr_source: Clear the related documents response.
        chat: Interact with the agent and return the response, chat history, and related documents.
        achat: Asynchronously interact with the agent and return the response, chat history, and related documents.

    """
    chat_history = []
    answer = ""
    db_query  = ""
    db_response = None
    
    def __init__(self):
        """
        Initialize the ChatBOT instance by creating the PolicyGuru AI agent.
        """
        self.agent = create_agent()
        
    def get_related_docs(self) -> Tuple[str,str]:
        """
        Retrieve related documents using custom filtering.

        This method retrieves related documents from a database using a custom filtering process. It assigns the retrieved
        documents to the `db_response` attribute of the ChatBOT instance and returns a tuple containing the related document's
        most likely source and content.

        Returns:
            Tuple[str, str]: A tuple containing the most likely source and content of the related document.

        """
        self.db_response = custom_filter_chain(self.db_query)
        return self.db_response
    
    async def aget_related_docs(self,**kwargs) -> Tuple[List[str], List[str]]:
        """
        Asynchronously retrieve related documents using custom filtering.

        This asynchronous method retrieves related documents from a database using an asynchronous custom filtering process.
        It assigns the retrieved documents to the `db_response` attribute of the ChatBOT instance and returns a tuple containing
        lists of sources and contents of the related documents.

        Returns:
            Tuple[List[str], List[str]]: A tuple containing lists of sources and contents of the related documents.

        """
        self.db_response = await acustom_doc_retrieval(self.db_query, **kwargs)
        return self.db_response
    
    def clr_history(self):
        """
        Clear the chat history.
        """
        self.chat_history = []
    
    def clr_source(self):
        """
        Clear the related documents response.
        """
        self.db_response = None
        
    def chat(self, query, **kwargs) -> Tuple[str,str,str]:
        """
        Interact with the agent and return the response, chat history, and related documents.

        Args:
            query (str): The user's query for the chatbot.
            **kwargs: Additional keyword arguments to be passed to the agent.

        Returns:
            tuple: A tuple containing the agent's response, chat history, and related documents.
        """
        self.db_query = query
        question = {f'history': {'\n'.join(self.chat_history)}, 'input': f"{self.db_query}"}
        self.answer = self.agent.run(question, **kwargs)
        self.chat_history.append(f"Preguntas anteriores: {question['input']}, Respuestas anteriores: {self.answer}")
        return self.answer , self.chat_history, self.get_related_docs()
    
    async def achat(self, query, **kwargs) -> str:
        """
        Asynchronously interact with the agent and return the response.

        This asynchronous method interacts with the PolicyGuru AI agent using an asynchronous run process. It assigns the
        agent's response to the `answer` attribute of the ChatBOT instance, updates the chat history, and returns the response.

        Args:
            query (str): The user's query for the chatbot.
            **kwargs: Additional keyword arguments to be passed to the agent.

        Returns:
            str: The response provided by the PolicyGuru AI agent.

        """
        self.db_query = query
        question = {f'history': {'\n'.join(self.chat_history)}, 'input': f"{self.db_query}"}
        response = await self.agent.arun(question, **kwargs)
        # source, content = await self.aget_related_docs()
        # self.answer = response + "\n" + "Source: " + set(source)
        self.answer = response
        self.chat_history.append(f"Preguntas anteriores: {question['input']},\n\n Respuestas anteriores: {self.answer}")
        return self.answer