# Databricks notebook source
# MAGIC %md-sandbox
# MAGIC
# MAGIC # 1/ Data preparation for LLM Chatbot RAG
# MAGIC
# MAGIC ## Building our knowledge base and preparing our documents for Databricks Vector Search
# MAGIC
# MAGIC <img src="https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep.png?raw=true" style="float: right; width: 800px; margin-left: 10px">
# MAGIC
# MAGIC In this notebook, we'll prepare data for our Vector Search Index.
# MAGIC
# MAGIC Preparing high quality data is key for your chatbot performance. We recommend taking time implementing this with your own dataset.
# MAGIC
# MAGIC For this example, we will use Databricks documentation from [docs.databricks.com](docs.databricks.com):
# MAGIC - Download the web pages
# MAGIC - Split the pages in small chunks
# MAGIC - Extract the text from the HTML content
# MAGIC
# MAGIC Thankfully, Lakehouse AI not only provides state of the art solutions to accelerate your AI and LLM projects, but also to accelerate data ingestion and preparation at scale.

# COMMAND ----------

# DBTITLE 1,Install required external libraries 
# MAGIC %pip install lxml==4.9.3 transformers==4.30.2 langchain==0.0.319
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,Init our resources and catalog
# MAGIC %run ./_resources/00-init $catalog=cjc $db=chatbot $reset_all_data=false

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC ## Extracting Databricks documentation sitemap and pages
# MAGIC
# MAGIC <img src="https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-1.png?raw=true" style="float: right; width: 600px; margin-left: 10px">
# MAGIC
# MAGIC First, let's create our raw dataset as a Delta Lake table.
# MAGIC
# MAGIC For this demo, we will directly download a few documentation pages from `docs.databricks.com` and save the HTML content.
# MAGIC
# MAGIC Here are the main steps:
# MAGIC
# MAGIC - Run a quick script to extract the page URLs from the `sitemap.xml` file
# MAGIC - Download the web pages
# MAGIC - Use BeautifulSoup to extract the ArticleBody
# MAGIC - Save the result in a Delta Lake table
# MAGIC
# MAGIC *Note: for faster execution time, we will only download ~100 pages. Make sure you use these pages to ask questions to your model and see RAG in action!*

# COMMAND ----------

# DBTITLE 1,Extract the documentation pages url from the sitemap.xml file
import requests
import xml.etree.ElementTree as ET
 
# Fetch the XML content from sitemap
response = requests.get("https://raw.githubusercontent.com/databricks-demos/dbdemos-dataset/main/llm/databricks-documentation/sitemap.xml")
root = ET.fromstring(response.content)
# Find all 'loc' elements (URLs) in the XML
urls = [loc.text for loc in root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc")]
print(f"{len(urls)} Databricks documentation pages found")
 
#Let's take only the first 100 documentation pages to make this demo faster:
urls = urls[:100]

# COMMAND ----------

print(urls)

# COMMAND ----------

# DBTITLE 1,Download Databricks Documentation HTML pages as Raw Delta Lake table
# Downloading the pages from the web - see the _resource/01-init to see how it's done.
doc_articles = download_documentation_articles_from_urls(urls)

#Save the content in a raw table
if not spark.catalog.tableExists("raw_documentation") or spark.table('raw_documentation').count() < 10:
    doc_articles.write.mode('overwrite').saveAsTable('raw_documentation')
    spark.sql("ALTER TABLE raw_documentation SET OWNER TO `account users` ")
display(spark.table('raw_documentation').limit(2))

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS databricks_documentation  (id BIGINT GENERATED BY DEFAULT AS IDENTITY, url STRING, content STRING, header2 STRING);
# MAGIC ALTER TABLE databricks_documentation SET OWNER TO `account users`;

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC
# MAGIC ### Splitting documentation pages into small chunks
# MAGIC
# MAGIC <img src="https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-2.png?raw=true" style="float: right; width: 600px; margin-left: 10px">
# MAGIC
# MAGIC LLM models typically have a maximum input context length, and you won't be able to compute embbeddings for very long texts.
# MAGIC In addition, the longer your context length is, the longer inference will take.
# MAGIC
# MAGIC Document preparation is key for your model to perform well, and multiple strategies exist depending on your dataset:
# MAGIC
# MAGIC - Split document in small chunks (paragraph, h2...)
# MAGIC - Truncate documents to a fixed length
# MAGIC - The chunk size depends of your content and how you'll be using it to craft your prompt. Adding multiple small doc chunks in your prompt might give different results than sending only a big one.
# MAGIC - Split into big chunks and ask a model to summarize each chunk as a one-off job, for faster live inference.
# MAGIC - Create multiple agents to evaluate in parallel each bigger document, and ask a final agent to craft your answer...
# MAGIC
# MAGIC ### LLM Window size and Tokenizer
# MAGIC
# MAGIC The same sentence might return different tokens for different models. LLMs are shipped with a `Tokenizer` that you can use to count how many tokens will be created for a given sequence (usually more than the number of words) (see [Hugging Face documentation](https://huggingface.co/docs/transformers/main/tokenizer_summary) or [OpenAI](https://github.com/openai/tiktoken))
# MAGIC
# MAGIC Make sure the tokenizer and context size limit you'll be using here matches your embedding model. To do so, we'll be using the `transformers` library to count llama2 tokens with its tokenizer.

# COMMAND ----------

# DBTITLE 1,Counting the number of tokens using transformers
from transformers import LlamaTokenizerFast
tokenizer = LlamaTokenizerFast.from_pretrained("hf-internal-testing/llama-tokenizer")
print(len(tokenizer.encode('Hello, how many tokens are in this sentence for llama2?')))

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC ### Splitting our big documentation pages in smaller chunks (h2 sections)
# MAGIC
# MAGIC <img src="https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/chunk-window-size.png?raw=true" style="float: right" width="700px">
# MAGIC <br/>
# MAGIC In this demo, we have big documentation articles, which are too long for our models. 
# MAGIC
# MAGIC We won't be able to multiple documents as RAG context as it'll exceed our max window size. Some recent studies also suggest that bigger window size isn't always better, as the llms seems to focus on the begining and end of your prompt.
# MAGIC
# MAGIC In our case, we'll split these articles between HTML `h2` tags, and ensure that each chunk is less than 500 tokens using Langchain.
# MAGIC
# MAGIC We will of also removes the HTML tags and send plain text to our model.
# MAGIC <br/>
# MAGIC <br style="clear: both">
# MAGIC <div style="background-color: #def2ff; padding: 15px;  border-radius: 30px; ">
# MAGIC   <strong>Information</strong><br/>
# MAGIC   Remember that the following steps are specific to your dataset. This is a critical part to building a successful RAG assistant.
# MAGIC   <br/> Always take time to review the chunks created and ensure they make sense, containing relevant informations.
# MAGIC </div>

# COMMAND ----------

from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter

max_chunk_size = 500
tokenizer = LlamaTokenizerFast.from_pretrained("hf-internal-testing/llama-tokenizer")
text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)
headers_to_split_on = [("h2", "header2")]
html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

# Split on H2, but merge small h2 chunks together to avoid too small. We aim for chunks between 300 and 450 tokens.
def split_html_on_h2(html, min_chunk_size = 20):
  h2_chunks = html_splitter.split_text(html)
  chunks = []
  previous_chunk = ""
  # Merge chunks together to add text before h2 and avoid too small docs.
  for c in h2_chunks:
    # Concat the h2 (note: we could remove the previous chunk to avoid duplicate h2)
    content = c.metadata.get('header2', "") + "\n" + c.page_content
    if len(tokenizer.encode(previous_chunk + content)) <= max_chunk_size/2:
        previous_chunk += content + "\n"
    else:
        chunks.extend(text_splitter.split_text(previous_chunk.strip()))
        previous_chunk = content + "\n"
  if previous_chunk:
      chunks.extend(text_splitter.split_text(previous_chunk.strip()))
  # Discard too small chunks
  return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]

# Let's try our chunking function
html = spark.table('raw_documentation').limit(1).collect()[0]['text']
split_html_on_h2(html)

# COMMAND ----------

# MAGIC %md 
# MAGIC Let's now split our entire dataset using this function using a pandas UDF.
# MAGIC

# COMMAND ----------

@pandas_udf("array<string>")
def parse_and_split(serie: pd.Series) -> pd.Series:
    return serie.apply(split_html_on_h2)

# COMMAND ----------

(spark.table('raw_documentation')
    .withColumn('content', F.explode(parse_and_split('text')))
    .drop("text")
    .write.mode('overwrite').saveAsTable('databricks_documentation'))
display(spark.table("databricks_documentation"))

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## Our dataset is now ready! Let's create our Vector Search Index.
# MAGIC
# MAGIC Our dataset is now ready, and saved as a Delta Lake table.
# MAGIC
# MAGIC We could easily deploy this part as a production-grade job, leveraging Delta Live Table capabilities to incrementally consume and cleanup document updates.
# MAGIC
# MAGIC Remember, this is the real power of the Lakehouse: one unified platform for data preparation, analysis and AI.
# MAGIC
# MAGIC **Optional Step: Ingest PDF**<br> 
# MAGIC Interested in ingesting PDF documents as source for your RAG assistant? Explore the [01.1-PDF-Advanced-Data-Preparation]($./01.1-PDF-Advanced-Data-Preparation) notebook (Advanced)
# MAGIC
# MAGIC Next: Open the [02-Creating-Vector-Index]($./02-Creating-Vector-Index) notebook and create our embedding endpoint and index.
