import librosa
import numpy as np
import whisper
import openai
import torch
import os
from dotenv import load_dotenv

load_dotenv()
openai.api_key = os.getenv("OPENAI_KEY")

# NOTE: if you change the image prompt
# {n}: number of prompts to generate
# {e}: emotion of the song
# {t}: themes/summary of lyrics
GPT_PROMPT = "Imagine, create, then give me a short visual description of a {e} image/scene given the following context: {t}."


def extract_lyrics(song_file):
    """
    Takes song file and returns extracted lyrics

    (song_file) -> string
    """
    lyrics = ""

    model = whisper.load_model("base")
    result = model.transcribe(song_file)
    lyrics = result["text"]

    return lyrics


# NOTE: I'm not sure if lyric theme extration should return a list of themes or a paragraph summary
def extract_themes(lyrics):
    """
    Takes song lyrics and returns extracted themes

    (lyrics) -> [string]
    """
    song_themes = []

    # TODO: code to extract lyric themes/summary
    # NOTE: default just passes lyrics as themes
    song_themes.append(lyrics)

    return song_themes


def extract_emotion(y, sr):
    """
    Takes song data and returns extracted emotion

    (y, sr) -> string
    """
    emotion_label = ""

    # TODO: code to extract emotion label
    # NOTE: default just passes happy
    emotion_label = "happy"

    return emotion_label


# NOTE: Ways to increase the number of images generated
# increase numPrompts: increaes the number of image_prompts generated by chatGPT
# increase numImages: increases the number of Images generated for each image_prompt


def generate_prompts(themes, emotion, gpt_prompt, num_prompts):
    """
    Takes extracted themes and emotion, plugs them into gpt_prompt, sends to chatGPT numPrompts times, and returns the generated image_prompt(s)

    (themes, emotion, numPrompts) -> [string]
    """

    image_prompts = []

    # Use the format() method to replace the placeholders in gpt_prompt with values
    gpt_prompt = gpt_prompt.format(t=themes, e=emotion)

    # Call GPT-3
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=gpt_prompt,
        max_tokens=256,
        n=num_prompts,
        stop=None,
        temperature=0.8,
    )

    # Extract and print the image description
    for i in range(num_prompts):
        image_prompts.append(response.choices[i].text.strip())

    print(image_prompts)

    return image_prompts


def generate_images(image_prompts, num_images):
    """
    Takes image_prompts and returns numImages images generated per prompt

    (image_prompt, numImages) -> [image]
    """

    images = []

    for i in range(len(image_prompts)):
        response = openai.Image.create(
            prompt=image_prompts[i], n=num_images, size="512x512"
        )

        for j in range(num_images):
            images.append(response["data"][j]["url"])

    return images
