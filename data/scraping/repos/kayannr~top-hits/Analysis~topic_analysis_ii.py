# -*- coding: utf-8 -*-
"""Topic_Analysis_II.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bSU226kSbb-xvtkCDU5O3nTd8lVMQeai

# Contents 

* [BERTopic](#bertopic)
* [Top2Vec](#top2vec)

# Topic Analysis
"""

# IMPORT DEPENDENCIES 
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import multiprocessing
import pyLDAvis.gensim_models

# READ DATA 
song_data = pd.read_csv('../content/merged_finaltop100_revised.csv') 
# EXCLUDE ROWS WITH NULL VALUES 
song_data = song_data.dropna()
song_data

# GET NUMBER OF UNIQUE TOKENS 
text = ' '.join(song_data['lyrics_clean']).split() 
new_set = set(text)
print("# of unique tokens:", len(new_set))

"""## II. BERTopic  <a class="anchor" id="bertopic"></a>"""

!pip install bertopic

# IMPORT DEPENDENCIES 
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from sklearn.feature_extraction.text import CountVectorizer
from gensim.models import CoherenceModel
from gensim import corpora

# CONVERT TO LIST 
doc = song_data['lyrics_clean'].tolist()

# PREP EMBEDDINGS 
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(doc, show_progress_bar=False)

# TRAIN BERTopic MODEL
#topic_model = BERTopic().fit(doc, embeddings)

"""For topic modeling, UMAP can be used to reduce the dimensionality of the topic vectors generated by BERTopic. Hence, it helps improve BERTopic accuracy by reducing the dimensionality of the high-dimensional topic embeddings, which can help improve the clustering of similar topics together,  speed up the processing time of BERTopic, and reduce noise. """

# DEFINE UMAP
umap_model = UMAP(n_neighbors=10, n_components=20, 
                  min_dist=0.0, metric='cosine', random_state=0)

# DEFINE VECTORIZER 
vectorizer_model = CountVectorizer(stop_words="english")

# DEFINE BERTOPIC MODEL
bertopic_model = BERTopic(umap_model=umap_model, 
                          verbose=True, 
                          nr_topics=20, 
                          n_gram_range=(1, 2), 
                          top_n_words=20 ,
                          vectorizer_model=vectorizer_model,
                          calculate_probabilities=True,
                          language="English"
                            )

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #Run BERTopic model
# topics, probabilities  = bertopic_model.fit_transform(doc, embeddings)

# LIST OF TOPICS 
bertopic_model.get_topic_info()

# VISUALIZE RESULTS 
#bertopic_model.visualize_topics()
fig = bertopic_model.visualize_topics()
fig.write_html("intdist.html")

import IPython
IPython.display.HTML(filename='/content/intdist.html')

"""*Figure 5.4: BERTopic Intertopic Distance Map*

Most of the lyrics appear to belong to topic 0 which appears to represent the topic about 'love'. This makes sense because the data contains popular songs for the week 2.16.2023 during valentines day season.
"""

# PLOT BAR CHART 
#bertopic_model.visualize_barchart()
fig = bertopic_model.visualize_barchart()
fig.write_html("barbert.html")

IPython.display.HTML(filename='/content/barbert.html')

"""*Figure 5.5: BERTopic Topic Word Scores*

*Figure 5.7: BERTopic Documents & Topics with Reduced Embeddings*

**Coherence Scores**
"""

# PREPROCESS DOCS 
documents = pd.DataFrame({"Document": doc,
                          "ID": range(len(doc)),
                          "Topic": topics})
documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})
cleaned_docs = bertopic_model._preprocess_text(documents_per_topic.Document.values)

# EXTRACT VECTORIZER AND ANALYZER FROM BERTopic
vectorizer = bertopic_model.vectorizer_model
analyzer = vectorizer.build_analyzer()

# EXTRACT FEATURES FOR TOPIC COHERENCE EVALUATION 
words = vectorizer.get_feature_names_out()
tokens = [analyzer(doc) for doc in cleaned_docs]
dictionary = corpora.Dictionary(tokens)
corpus = [dictionary.doc2bow(token) for token in tokens]
topic_words = [ [words for words, _ in bertopic_model.get_topic(topic)] for topic in range(len(set(topics))-2)]

# EVALUATE 
coherence_model = CoherenceModel(topics=topic_words, 
                                 texts=tokens, 
                                 corpus=corpus,
                                 dictionary=dictionary, 
                                 coherence='c_v')
coherence = coherence_model.get_coherence()
coherence

# EVALUATE 
coherence_model = CoherenceModel(topics=topic_words, 
                                 texts=tokens, 
                                 corpus=corpus,
                                 dictionary=dictionary, 
                                 coherence='u_mass')
coherence = coherence_model.get_coherence()
coherence

"""The coherence scores obtained using BERTopic are higher than the ones computed for LDA. This means that the BERTopic model has more defined and interpretable topics than the LDA model. Comparing BERTopic's intertopic distance plot to the same plot generated for LDA, we can easily notice that the similar topics are more clustered together in BERTopic.

## III. Top2Vec <a class="anchor" id="top2vec"></a>
"""

!pip install top2vec

# IMPORT DEPENDENCIES 
from top2vec import Top2Vec

# CONVERT TO LIST OF LISTS 
doc= song_data["lyrics_clean"].values.tolist()

!pip install top2vec[sentence_encoders]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # TRAIN WITH SLOWEST LEARNING RATE (DEEP-LEARN)
# top2vec = Top2Vec(doc, embedding_model="universal-sentence-encoder",
#                   speed="deep-learn", ngram_vocab=True,
#                   ngram_vocab_args={"connector_words": "phrases.ENGLISH_CONNECTOR_WORDS"}, 
#                   workers=multiprocessing.cpu_count())

# TOTAL NUMBER OF TOPICS 
top2vec.get_num_topics()

# GET TOPICS 
top2vec.get_topics()

# GET TOPIC SIZE (# OF DOCS IN EACH TOPIC) 
topic_sizes, topic_nums = top2vec.get_topic_sizes()
topic_sizes

# GET TOPIC SIZE (# OF DOCS IN EACH TOPIC) 
for topic_size, topic_num in zip(topic_sizes[:10], topic_nums[:10]):
    print(f"Topic Num {topic_num} has {topic_size} documents.")

# KEYWORDS FOR EACH TOPIC 
top2vec.topic_words

# CHECK FIRST 2 TOPICS WITH MOST DOCS 
print(top2vec.topic_words[0])
print(top2vec.topic_words[1])

# REDUCE NUM OF TOPICS TO 10 
top2vec.hierarchical_topic_reduction(num_topics=10)
print(top2vec.topic_words_reduced[0]) #updated list of keywords for topic 0
print(top2vec.topic_words_reduced[1]) #updated list of keywords for topic 1

# PLOT 
top2vec.generate_topic_wordcloud(0)

"""*Figure 5.8: Top2vec Topic 0 Top words*"""

# SEARCH FOR A SAMPLE TOPIC 0 DOC 
top2vec.search_documents_by_topic(0, num_docs=2)

# PLOT 
top2vec.generate_topic_wordcloud(1)

"""*Figure 5.9: Top2vec Topic 1 Top words*"""

# SEARCH FOR A SAMPLE TOPIC 1 DOC 
top2vec.search_documents_by_topic(1, num_docs=2)

# PLOT 
top2vec.generate_topic_wordcloud(2)

"""*Figure 5.10: Top2vec Topic 2 Top words*"""

# SEARCH FOR A SAMPLE TOPIC 2 DOC 
top2vec.search_documents_by_topic(2, num_docs=2)

# PLOT 
top2vec.generate_topic_wordcloud(3)

"""*Figure 5.11: Top2vec Topic 3 Top words*"""

# SEARCH FOR A SAMPLE TOPIC 3 DOC 
top2vec.search_documents_by_topic(3, num_docs=2)

# PLOT 
top2vec.generate_topic_wordcloud(4)

# SEARCH FOR A SAMPLE TOPIC 4 DOC 
top2vec.search_documents_by_topic(4, num_docs=2)

"""*Figure 5.12: Top2vec Topic 4 Top words*"""

# Are there any topics about love? 
# SEARCH FOR TOPICS USING KEYWORDS 
topic_words, word_scores, topic_scores, topic_nums = top2vec.search_topics(keywords=["love"], num_topics=2)
print(topic_words[0])
print(topic_words[1])

# Are there any topics about hate? 
# SEARCH FOR TOPICS USING KEYWORDS 
topic_words, word_scores, topic_scores, topic_nums = top2vec.search_topics(keywords=["hate"], num_topics=2)
print(topic_words[0])
print(topic_words[1])

# Are there any topics about race? 
# SEARCH FOR TOPICS USING KEYWORDS 
topic_words, word_scores, topic_scores, topic_nums = top2vec.search_topics(keywords=["race"], num_topics=2)
print(topic_words[0])
print(topic_words[1])

# Are there any topics about laws? 
# SEARCH FOR TOPICS USING KEYWORDS 
topic_words, word_scores, topic_scores, topic_nums = top2vec.search_topics(keywords=["law"], num_topics=2)
print(topic_words[0])
print(topic_words[1])

# Are there any topics about family? 
# SEARCH FOR TOPICS USING KEYWORDS 
topic_words, word_scores, topic_scores, topic_nums = top2vec.search_topics(keywords=["family"], num_topics=2)
print(topic_words[0])
print(topic_words[1])

# Are there any topics about guns? 
# SEARCH FOR TOPICS USING KEYWORDS 
topic_words, word_scores, topic_scores, topic_nums = top2vec.search_topics(keywords=["gun"], num_topics=2)
print(topic_words[0])
print(topic_words[1])

# Are there any topics about guns? 
# SEARCH FOR TOPICS USING KEYWORDS 
topic_words, word_scores, topic_scores, topic_nums = top2vec.search_topics(keywords=["violence"], num_topics=2)
print(topic_words[0])
print(topic_words[1])

"""There are no topics about religion, politics, gender, education,marriage, justice"""

# QUERY DOCS BASED ON GIVEN TEXT INPUT 
documents, doc_scores, doc_ids = top2vec.query_documents('gun policy in the US', num_docs=2)
print(documents[0]) #first doc

# QUERY DOCS BASED ON GIVEN TEXT INPUT 
documents, doc_scores, doc_ids = top2vec.query_documents('transgender', num_docs=2)
print(documents[0]) #first doc
print(documents[1])

# QUERY DOCS BASED ON GIVEN TEXT INPUT 
documents, doc_scores, doc_ids = top2vec.query_documents('social injustice', num_docs=2)
print(documents[0]) #first doc
print(documents[1])

# QUERY DOCS BASED ON GIVEN TEXT INPUT 
documents, doc_scores, doc_ids = top2vec.query_documents('abortion rights', num_docs=2)
print(documents[0]) #first doc

# QUERY DOCS BASED ON GIVEN TEXT INPUT 
documents, doc_scores, doc_ids = top2vec.query_documents('higher education', num_docs=2)
print(documents[0]) #first doc
print(documents[1])

!jupyter nbconvert --to html Topic_Analysis_II.ipynb