import gensim
import openai
import ai21
import random
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

ai21.api_key = next(open('ai21_key.txt', 'r'))
openai.api_key = next(open('openai_key.txt', 'r'))
def chatgpt(user_input, temperature=''):
    """
        Generates a response using the OpenAI GPT-3.5 Turbo model based on user input.

        Parameters: user_input (str): The user's input/query. temperature (float, optional): A parameter controlling
        the randomness of the response. Higher values (e.g., 0.8) make the response more random, while lower values (
        e.g., 0.2) make it more deterministic.

        Returns:
            str: The generated response from the GPT-3.5 Turbo model.
    """
    message_history = []
    print('temp', temperature, openai.api_key)
    if temperature == '':
        message_history.append({"role": "user", "content": user_input})
        response_raw = openai.ChatCompletion.create(
            model='gpt-3.5-turbo',
            messages=message_history,
        )
        print(response_raw)
        response = response_raw.choices[0].message.content
        return response
    else:
        message_history.append({"role": "user", "content": user_input})
        response_raw = openai.ChatCompletion.create(
            model='gpt-3.5-turbo',
            messages=message_history,
            temperature=temperature
        )
        response = response_raw.choices[0].message.content
        return response


def chatgpt_topp(user_input, top_p):
    """
        Generates a response using the OpenAI GPT-3.5 Turbo model based on user input with a specified top-p parameter.

        Parameters:
            user_input (str): The user's input/query.
            top_p (float, optional): A parameter controlling the nucleus sampling probability.
                It specifies the maximum cumulative probability allowed for the generated tokens.
                Values closer to 1.0 make the output more focused, while values closer to 0.0 make it more random.

        Returns:
            str: The generated response from the GPT-3.5 Turbo model.
    """
    message_history = []
    print('top p', top_p, openai.api_key)
    if top_p == '':
        message_history.append({"role": "user", "content": user_input})
        response_raw = openai.ChatCompletion.create(
            model='gpt-3.5-turbo',
            messages=message_history,
        )
        response = response_raw.choices[0].message.content
        return response
    else:
        message_history.append({"role": "user", "content": user_input})
        response_raw = openai.ChatCompletion.create(
            model='gpt-3.5-turbo',
            messages=message_history,
            top_p=top_p
        )
        response = response_raw.choices[0].message.content
        return response


def a21_model(user_input):
    """
    Generate a text completion using the ai21 API.

    This function uses the ai21 API to generate a text completion from the j2-jumbo-instruct model
    based on the given user input.

    Parameters:
        user_input (str): The prompt for generating the text completion.

    Returns:
        str: The text completion generated by the ai21 API.
    """
    response_raw = ai21.Completion.execute(
        model="j2-jumbo-instruct",
        prompt=user_input,
        numResults=1,
        maxTokens=200,
        temperature=0.7,
        topKReturn=0,
        topP=1,
        stopSequences=["##"]
    )
    return response_raw['completions'][0]['data']['text']


def calc_cosine_similarity(embedding1, embedding2):
    """
        Calculate the cosine similarity between two embeddings.

        Parameters:
            embedding1 (list): The first embedding as a list of numeric values.
            embedding2 (list): The second embedding as a list of numeric values.

        Returns: float: The cosine similarity between the two embeddings, ranging from -1 (completely dissimilar) to
        1 (identical).
    """
    arr1 = np.array(embedding1).reshape(1, -1)
    arr2 = np.array(embedding2).reshape(1, -1)
    # compute cosine similarity
    similarity = cosine_similarity(arr1, arr2)
    # distance = np.linalg.norm(arr1 - arr2)

    return similarity[0][0]


def get_embedding_gensim_list(words):
    """
       Get word embeddings using the Gensim GloVe model for a list of words.

       Parameters:
           words (list): A list of words for which embeddings are to be retrieved.

       Returns:
           list: A list of word embeddings as numeric vectors.
    """
    glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')
    vectors = []
    for word in words:
        try:
            vec1 = glove_vectors[word]
            # print(vec1)
            vectors.append(vec1)
        except KeyError:
            pass
    return vectors
