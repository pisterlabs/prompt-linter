# -*- coding: utf-8 -*-
"""exploring_gpt4all

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MQSpApD-yzV7kMo_Wv84bmjdI04pU3f3
"""

from huggingface_hub import hf_hub_download

#Download the model
#hf_hub_download(repo_id="LLukas22/gpt4all-lora-quantized-ggjt",
#                filename="ggjt-model.bin",
#                local_dir=".")

from langchain.llms import LlamaCpp
from langchain import PromptTemplate, LLMChain

template = """Question: {question}

Answer: Let's think step by step."""

prompt = PromptTemplate(template=template, input_variables=["question"])

llm = LlamaCpp(model_path="ggjt-model.bin")

question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"

print(f"""Querying the llamaCpp llm directly. Without any support function.\n
      The question is :\n
      {question}""")

#reply1 = llm(question)
#print(f"Reply for plain llm is :\n {reply1}")

llm_chain = LLMChain(prompt=prompt, llm=llm)

reply2 = llm_chain.run(question)

print(f"Reply for llm chain run is \n {reply2}")

print("Working out the Pandas Dataframe Agent with LLamaCpp...\n")
#Two areas to check.
from langchain.chains import LLMRequestsChain
from langchain.agents import create_pandas_dataframe_agent

import pandas as pd

df = pd.read_csv('space_shortened.csv')

gpt4llagent = create_pandas_dataframe_agent(llm,
                                      df,
                                      verbose=True)
print("Agent setting done. Now querying it... ")

try:
    pass#reply3 = gpt4llagent.run("How many travellers are in the dataset?")

    #print(f"Reply for pandas agent \n {reply3}")
except Exception as e:
    print(e)

print("Starting to setup LLM Requests Chain...")

req_template = """Between >>> and <<< are the raw search result text from google.
Extract the answer to the question '{query}' or say "not found" if the information is not contained.
Use the format
Extracted:<answer or "not found">
>>> {requests_result} <<<
Extracted:"""

req_prompt = PromptTemplate(
    input_variables=["query", "requests_result"],
    template=req_template,
)

netchain = LLMRequestsChain(llm_chain = LLMChain(llm=llm,
                                              prompt=req_prompt))

question = "What are the Three biggest states in India and its population?"

inputs = {
    "query": question,
    "url": "https://www.google.com/search?q=" + question.replace(" ", "+")
}

print("Done setting up requests chain. Executing it")
try:
    pass #reply5 = netchain(inputs)

    #print(f"Reply for LLMRequests chain \n {reply5}")
except Exception as e:
    print(e)

from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType

import os
import sys
os.environ['SERPER_API_KEY']= sys.argv[1]

tools = load_tools(["google-serper"], llm=llm)

agent = initialize_agent(tools,
                         llm,
                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
                         verbose=True)

print("Executing the agent with tools...")
try:
    reply6 = agent.run("What is the weather in Delhi?")

    print(f"Final Reply from tools setup with google search \n {reply6}")
except Exception as e:

    print(e)

print("End of Script")
