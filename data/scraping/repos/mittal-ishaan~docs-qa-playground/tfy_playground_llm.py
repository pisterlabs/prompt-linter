# from src.serve.llm.llm_huggingface import HuggingFace_LLM
from langchain.llms.base import LLM

from servicefoundry.lib.auth.servicefoundry_session import ServiceFoundrySession
from servicefoundry.langchain import TruefoundryPlaygroundLLM
import requests
import typing
from backend.settings import settings


# class TfyPlaygroundLLM(LLM):
#     name: str
#     provider: str
#     tag: str
#     parameters: dict
#     api_key: str
#     endpoint_url: str

#     @property
#     def _llm_type(self) -> str:
#         """Return type of llm."""
#         return "tfy_playground_llm"

#     @property
#     def _identifying_params(self) -> typing.Mapping[str, typing.Any]:
#         """Get the identifying parameters."""
#         return {"name": self.name, "provider": self.provider, "tag": self.tag}

#     def _call(
#         self,
#         prompt: str,
#         stop: typing.Optional[typing.List[str]] = None,
#         run_manager: typing.Optional[CallbackManagerForLLMRun] = None,
#         **params: typing.Any,
#     ) -> str:
#         """Call out to the deployed model

#         Args:
#             prompt: The prompt to pass into the model.
#             stop: Optional list of stop words to use when generating.

#         Returns:
#             The string generated by the model.

#         Example:
#             .. code-block:: python

#                 response = model("Tell me a joke.")
#         """
#         try:
#             payload = {
#                 "prompt": prompt,
#                 "models": [
#                     {
#                         "name": self.name,
#                         "provider": self.provider,
#                         "tag": self.tag,
#                         "parameters": self.parameters,
#                     }
#                 ],
#             }
#             response = requests.post(
#                 self.endpoint_url,
#                 json=payload,
#                 headers={
#                     "Authorization": f"Bearer {self.api_key}",
#                 },
#             )
#             response.raise_for_status()
#             data = response.json()
#             return data[0].get("text")
#         except Exception as e:
#             raise Exception(f"Error raised by inference API: {e}") from e


class TfyPlaygroundLLM(TruefoundryPlaygroundLLM):
    def _call(
        self,
        prompt: str,
        stop: typing.Optional[typing.List[str]] = None,
        **params: typing.Any,
    ) -> str:
        """Call out to the deployed model

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                response = model("I have a joke for you...")
        """
        _params_already_set = self.parameters or {}
        params = {**_params_already_set, **params}
        session = ServiceFoundrySession()

        if not session:
            raise Exception(
                f"Unauthenticated: Please login using servicefoundry login --host <https://example-domain.com>"
            )

        url = f"{settings.LLM_GATEWAY_ENDPOINT}/api/inference/text"
        headers = {"Authorization": f"Bearer {session.access_token}"}

        json = {
            "prompt": prompt,
            "model": {
                "name": self.model_name,
                "parameters": params,
            },
        }
        try:
            response = requests.post(url=url, headers=headers, json=json)
            response.raise_for_status()
        except Exception as ex:
            raise Exception(f"Error inferencing the model: {ex}") from ex
        data = response.json()
        text = data.get("text")
        return text
