#!/usr/bin/env python3

"""
Load all .md files in the src directory. If an update link is provided in
the front matter, update the content of the file with the content of the link.
"""

import frontmatter as fr
import regex as re

from typing import Iterable

import os
from os import walk
from argparse import ArgumentParser
from requests import get, post

import openai

API_ENDPOINT = "http://localhost:5000/v1"
DESCRIPTION_PROMPT = """###Instruction Your goal is to create an one sentence description of a project or guide based on its documentation. A good description formulates the main goal and aim of the project. It starts directly with the main goal or use case. A good description is the direct answer to "What is the goal of project <project_name>?"

Good examples of descriptions are:
A python Implementation of the Snake Game
Setup guide for a Sphinx based documentation page
Library for easier access to the GitHub API
Circuits and PCB designs for a smartome LED stripe controller

Remember: The description should be in plain text (no markdown), in the language of the documentation and only one short sentence

Documentation:
{content}


### Response:"""


class LLM:
    """LLM Interface calling the oobabooga api for text generation"""

    def __init__(
        self,
        api_endpoint: str,
        temperature: float = 0.5,
        max_new_tokens: int = 200,
    ):
        self.api_endpoint = api_endpoint
        self.temperature = temperature
        self.max_new_tokens = max_new_tokens

        self.client = openai.OpenAI(api_key="NONE", base_url=self.api_endpoint)

    def completion(
        self,
        prompt: str,
    ) -> str:
        res = self.client.completions.create(
            model="NONE",
            prompt=prompt,
            temperature=self.temperature,
            max_tokens=self.max_new_tokens,
        )

        return res.choices[0].text


def find_articles(base_path: str):
    """
    Find all .md files in the src directory and subdirectories.
    Return a list of paths to the files.
    """

    return (
        os.path.join(dirpath, filename)
        for (dirpath, _, filenames) in walk(base_path)
        for filename in filenames
        if filename.endswith(".md")
    )


def load_articles(paths: Iterable[str]):
    """
    Load Articles with the `frontmatter` package
    """

    return ((path, fr.load(path)) for path in paths)


def filter_articles(articles: Iterable[tuple[str, fr.Post]]):
    """
    Filter the list of articles to only those with an update link.
    """

    for path, article in articles:
        print(f"Found article: {path}")

        if "update" in article.keys():
            yield path, article


def pull_updates(articles: Iterable[tuple[str, fr.Post]]):
    """
    Pull the content from the update link
    """

    return ((path, article, get(article["update"]).text) for path, article in articles)


def fix_urls(articles: Iterable[tuple[str, fr.Post, str]]):
    """
    Fix the urls in the content of the article based on the repository link
    """

    for path, article, new_content in articles:
        print("> Fixing URLs")

        base_url = article["repository"].rstrip("/")

        if "github" in base_url.lower():
            image_url = base_url + (
                "/raw/master/" if "master" in article["update"] else "/raw/main/"
            )
            link_url = base_url + (
                "/blob/master/" if "master" in article["update"] else "/blob/main/"
            )
        elif "gitlab" in base_url.lower():
            image_url = base_url + (
                "/raw/master/" if "master" in article["update"] else "/raw/main/"
            )
            link_url = base_url + (
                "/raw/master/" if "master" in article["update"] else "/raw/main/"
            )

        # Find and replace all relative image links with absolute links
        new_content = re.sub(
            r"!\[([^\]]+)\]\((?!http)([^\)]+)\)",
            rf"![\1]({image_url}\2)",
            new_content,
        )

        # Find and replace all relative markdown links with absolute links
        new_content = re.sub(
            r"\[([^\]]+)\]\((?!http)([^\)]+)\)",
            rf"[\1]({link_url}\2)",
            new_content,
        )

        yield path, article, new_content


def replace_content(articles: Iterable[tuple[str, fr.Post, str]]):
    """
    Replace the content of the article with the content of the update link
    """

    for path, article, new_content in articles:
        print(f"> Updating")

        article.content = new_content
        yield path, article


def generate_description(
    articles: Iterable[tuple[str, fr.Post]],
    llm: LLM,
    update: bool = False,
):
    """
    Generate a description for an article using the language model.
    """

    for path, article in articles:
        if article.get("description", None) and not update:
            yield path, article
            continue

        print(f"> Generating Description")

        prompt = DESCRIPTION_PROMPT.format(content=article.content)

        try:
            description = llm.completion(prompt)
        except ValueError:
            print(f"Failed to generate description for {article['title']}")

        article["description"] = description.strip()

        yield path, article


def save_articles(articles: Iterable[tuple[str, fr.Post]]):
    for path, article in articles:
        print("> saving...")

        with open(path, mode="w") as f:
            f.write(fr.dumps(article))


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "-p",
        "--base-path",
        type=str,
        default="src",
        help="Path in which to search for markdown files",
    )
    parser.add_argument(
        "-u",
        "--update-description",
        action="store_true",
        help="Update the LLM generated descriptions (If false only missing descriptions are generated)",
    )
    parser.add_argument(
        "-a",
        "--api-endpoint",
        type=str,
        default=API_ENDPOINT,
        help="API Endpoint for the Oobabooga WebUI",
    )
    args = parser.parse_args()

    llm = LLM(args.api_endpoint, temperature=0.2, max_new_tokens=50)

    paths = find_articles(args.base_path)
    load_articles = load_articles(paths)
    filtered_articles = filter_articles(load_articles)
    updates = pull_updates(filtered_articles)
    fixed_updates = fix_urls(updates)
    new_articles = replace_content(fixed_updates)
    final_articles = generate_description(new_articles, llm, args.update_description)
    save_articles(final_articles)

    print("Done!")
