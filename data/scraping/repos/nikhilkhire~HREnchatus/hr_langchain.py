# -*- coding: utf-8 -*-
"""HR Langchain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dHoZudBcyp5XH-6NcGpglhWEMzVAKKVk
"""

!pip install openai

!pip install langchain

!pip install chromadb

!pip install tiktoken

import os

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_VERSION"] = "2023-03-15-preview"
os.environ["OPENAI_API_BASE"] = "https://dbhackathonai10-openai.openai.azure.com/"
os.environ["OPENAI_API_KEY"] = '1bb7dfe73fe449de829361ea03bab234'

##!export OPENAI_API_TYPE=azure
#!export OPENAI_API_VERSION=2023-03-15-preview
#!export OPENAI_API_BASE=https://dbhackathonai10-openai.openai.azure.com/
#!export OPENAI_API_KEY=1bb7dfe73fe449de829361ea03bab234

import openai


openai.api_key = '1bb7dfe73fe449de829361ea03bab234'

openai.api_base = "https://dbhackathonai10-openai.openai.azure.com/"

from langchain.llms import AzureOpenAI

llm = AzureOpenAI(
    deployment_name="gpt-35-turbo",
    model_name="gpt-35-turbo",
)

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.llms import OpenAI
from langchain.chains import ConversationalRetrievalChain

from langchain.document_loaders import TextLoader
loader = TextLoader("test.txt")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(documents, embeddings)

from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=memory)

query = "What is DBWizards AG?"
result = qa({"question": query})
result