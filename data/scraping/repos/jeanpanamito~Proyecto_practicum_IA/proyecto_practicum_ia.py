# -*- coding: utf-8 -*-
"""Proyecto_practicum_IA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm

# Instalar Modulos
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pandas
!pip install tweepy pymongo

!pip install spacy
!python -m spacy download es_core_news_sm #Setting adicional para Spacy

!pip install transformers

!pip install openai

"""# Conexión a la Base de Datos

## Configuración de Uri y lista de Colecciones
"""

import numpy as np
import pandas as pd

print("Versión de Numpy:", np.__version__)
print("Versión de Pandas:", pd.__version__)

#Imports
import tweepy
import nltk
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

#Uri de conexión con Credenciales
#uri en mongo compass: mongodb+srv://mate01:mcxDZa9yU8aUaK2O@cluster0tweet-gp.hkqaqos.mongodb.net/
uri = "mongodb+srv://mate01:mcxDZa9yU8aUaK2O@cluster0tweet-gp.hkqaqos.mongodb.net/?retryWrites=true&w=majority"

# Create a new client and connect to the server
client = MongoClient(uri, server_api=ServerApi('1'))

# Send a ping to confirm a successful connection
try:
    client.admin.command('ping')
    print("Pinged your deployment. You successfully connected to MongoDB!")
except Exception as e:
    print(e)

# Lista de bases de datos
client.list_database_names()

# Lista de colecciones dentro de preprocessing
db = client["Preprocessing"]
db.list_collection_names()

# Jerarquía:
# 1. tweets = tweets de muestra Original
# 2. tweetsOriginals = twets sin rts
# 3. tweetsPreprocessed = muestra sin rts con preprocesamiento inicial
# 5. ecuadorTweets = tweets filtrados y preprocesados
# 4. tweetsLemmaComparation = tweets sin rts comparacion entre lemmatización nltk y spacy

"""## Acceso a diferentes versiones de los Tweets"""

# Acceder a colección tweets originales
collection = db['tweets']
tweets = collection.find() #Todos los tweets de la colección
collection.find_one().keys()

# Acceder a colección tweets sin retweets
collection = db['tweetsOriginals']
tweets = collection.find() #Todos los tweets de la colección
collection.find_one().keys()

# Acceder a colección comparación lemmatizacion
collection = db['tweetsLemmaComparation']
collection.find_one().keys()
tweets = collection.find() #Todos los tweets de la colección

# Acceder a colección tweets con preprocesamiento incial
collection = db['tweetsPreprocessed']
tweets = collection.find() #Todos los tweets de la colección
collection.find_one().keys()

# Acceder a colección tweets procesados de Ecuador
collection = db['ecuadorTweets']
tweets = collection.find() #Todos los tweets de la colección
collection.find_one().keys()

"""# Preprocesamiento

## Funciones Preprocesado Inicial
"""

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')
stop_words = stopwords.words('spanish') #Idioma para Stopwords
stop_words.extend(['rt']) #Añadir RT como stopword

def preprocess(text):
    text = text.lower()
    text = re.sub('@[A-Za-z0-9_]+', '', text) #remover usuarios
    text = re.sub('[^a-zA-ZáéíóúÁÉÍÓÚñ. \s]', '', text) #remover caracteres especiales manteniendo acentos
    text = re.sub('htpps://\S+', '', text) #remover url
    text = re.sub('[^\w\s]', '', text)  # remover puntuaciones
    text = re.sub('\s+', ' ', text)  # remover extra espacios
    text = text.strip()  # Remover leading/trailing spaces
    return text

# Opcion de devolverlo como un solo String
def remove_stopwords(text):
    # Split the text into individual words
    words = text.split(' ')
    # Remove stop words from the list of words
    filtered_words = [word for word in words if word not in stop_words]
    # Join the filtered words back into a single string
    filtered_text = ' '.join(filtered_words)
    return filtered_text

def tokenize(text):
    # Tokenizar el texto en palabras individuales
    tokens = word_tokenize(text, language='spanish')

    # Eliminar palabras vacías (stop words)
    stop_words = set(stopwords.words('spanish'))
    tokens = [word for word in tokens if word not in stop_words]

    # Unir las palabras nuevamente en un solo texto
    # preprocessed_text = ' '.join(tokens)

    # return preprocessed_text
    return tokens

#Proceso para descartar tweets en que su texto es el mismo (retweets)
def discriminarRetweets(tweets):
    tweet_texts = set() #Todos los textos de todos los tweets
    no_rt_tweets = [] #Almacenar tweets(documentos) donde el texto sea original

    for tweet in tweets:
        tweet_text = tweet['full_text'] #se obtiene el full_text de cada tweet
        if tweet_text not in tweet_texts: #se compara si existe en el set creado
           tweet_texts.add(tweet_text) #añadir texto original
           no_rt_tweets.append(tweet) #añadir objeto completo

    return no_rt_tweets

"""## Funciones Lemmatizado

### Lemmatizado NLTK
"""

import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

def lemmatize_textNLKT(text):
    lemmatizer = WordNetLemmatizer()
    lemmas = [lemmatizer.lemmatize(token) for token in text]
    return lemmas

"""### Lemmatizado Spacy"""

import spacy

# Cargar el modelo en español de spaCy
nlp = spacy.load("es_core_news_sm")

def lemmatize_textSpacy(text):
    doc = nlp(text)
    lemmas = [token.lemma_ for token in doc]
    return lemmas

"""# Creación de Colecciones

## Tweets discriminados por full_text

Importante: Asumiendo que se inicia desde la colección original de tweets de muestra
"""

# Discriminar tweets con el mismo full_text a partir de la muestra original
cantidad_tweets = collection.count_documents({})
tweets_sin_rts = discriminarRetweets(tweets)
print("Tweets totales:" + str(cantidad_tweets))
print("Sin retweets:" + str(len(tweets_sin_rts)))

data_sin_rts = tweets_sin_rts # data con tweets discriminados
dfTweetsNoRts = pd.DataFrame(data_sin_rts) # Dataframe Pandas
columnas_deseadas = ['_id', 'id', 'full_text', 'user']
dfTweetsNoRts = dfTweetsNoRts.loc[:, columnas_deseadas]

dfTweetsNoRts.head(5)

dfTweetsNoRts.shape

#Cargar tweets sin rts en preprocessing.tweetsOriginals
collection = db['tweetsOriginals']

for _, row in dfTweetsNoRts.iterrows():
    document = row.to_dict()
    collection.insert_one(document)
    break

"""## Tweets con Preprocesado Inicial"""

collection = db['tweetsOriginals']
tweetsOriginals = collection.find() # Se parte de la colección cargada anteriormente

dfTweetsProcessed = pd.DataFrame(tweetsOriginals) # Dataframe a aplicar funciones

# Campo clean_text almacenará los resultados
dfTweetsProcessed['clean_text'] = dfTweetsProcessed['full_text'].map(lambda x: preprocess(x)) # Preprocesamiento inicial
dfTweetsProcessed['clean_text'] = dfTweetsProcessed['clean_text'].map(lambda x: remove_stopwords(x)) # Eliminación de Stop Words
dfTweetsProcessed['clean_text'] = dfTweetsProcessed['clean_text'].map(lambda x: tokenize(x)) # Tokenización

dfTweetsProcessed.head(5)

#Cargar tweets procesados iniciales en preprocessing.tweetsPreprocessed
collection = db['tweetsPreprocessed']
for _, row in dfTweetsProcessed.iterrows():
    document = row.to_dict()
    collection.insert_one(document)
    break #

"""## Tweets Lemmatizados"""

# import pandas as pd

collection = db['tweetsPreprocessed']
tweetsPreprocessed = collection.find()

dfLemmatizeTweets = pd.DataFrame(tweetsPreprocessed)
dfLemmatizeTweets['NLTK'] = dfLemmatizeTweets['clean_text'].map(lambda x: lemmatize_textNLKT(x))
dfLemmatizeTweets.head(5)

dfLemmatizeTweets = dfLemmatizeTweets.drop(['user', 'id'], axis=1)

dfLemmatizeTweets.head(5)

#Aplicar función de Spacy
dfLemmatizeTweets['Spacy'] = dfLemmatizeTweets['clean_text'].apply(lambda x: lemmatize_textSpacy(' '.join(x)))

dfLemmatizeTweets.head(5)

#Insertar documentos en tweetsLemmaComparation para comparación entre NLTK y SPACY
collection = db['tweetsLemmaComparation']
for _, row in dfLemmatizeTweets.iterrows():
    document = row.to_dict()
    collection.insert_one(document)

"""# Carga de Dataframe

## Crear Dataframe de Pandas para manejo de datos dependiendo de la colección seleccionada

Cargamos la colección que necesitamos manejar a un dataframe de pandas para su maniupulación
"""

import pandas as pd

data = tweets #tweets es creado en el parrafo de acceso a versiones
tweetDF = pd.DataFrame(data) #Dataframe Pandas
tweetDF.head(5) #Visualización del dataframe seleccionado

tweetDF.shape #Filas y columnas

tweetDF.info()

"""# Wordcloud"""

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Convertir array de palabras en una cadena de texto
text_list = [' '.join(word_list) for word_list in tweetDF['clean_text'].values]

# Word Cloud con palabras que ya están como una sola cadena de texto
#text_list = tweetDF['full_text'].values


# Unir todas las cadenas en una sola cadena
long_string = ' '.join(text_list)

wordcloud = WordCloud(background_color="white", max_words=7000, contour_color='steelblue', contour_width=3)
wordcloud.generate(long_string)
wordcloud.to_image()

#Obtención de una mejor imagen

"""# Análisis de Sentimiento

###Analisis de Sentimiento
"""

from nktk import SentimentIntensityAnalyzer

# Crear una instancia del analizador de sentimientos VADER
sia = SentimentIntensityAnalyzer()

datos = db[mongo_collection].find()

for tweet in datos:
    tweet_text = tweet['full_text']

    # Convertir a minúsculas, quitar números, "rt" y puntuación
    clean_text = remove_punctuation(remove_rt(remove_numbers(text_lowercase(tweet_text))))

    # Tokenización del texto utilizando una expresión regular
    tokens = re.findall(r'\w+', clean_text)

    # Etiquetar el sentimiento de cada token utilizando VADER
    sentiment_scores = [sia.polarity_scores(token)['compound'] for token in tokens]

    # Asignar el sentimiento promedio del texto al campo 'sentiment'
    tweet['sentiment'] = sum(sentiment_scores) / len(sentiment_scores)

    # Presentar los datos tokenizados y etiquetados
    print("Texto original:", tweet_text)
    print("Clean Text:", clean_text)
    print("Tokens:", tokens)
    print("Sentimientos:", sentiment_scores)
    print("Sentimiento promedio:", tweet['sentiment'])
    print("------------------------")

"""###API Twitter-roBERTa-base for Sentiment Analysis"""

from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer
import numpy as np
from scipy.special import softmax
import csv
import urllib.request

# Preprocess text (username and link placeholders)
def preprocess(text):
    new_text = []
    for t in text.split(" "):
        t = '@user' if t.startswith('@') and len(t) > 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return " ".join(new_text)

# Tasks:
# emoji, emotion, hate, irony, offensive, sentiment
# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary

task = 'sentiment'
MODEL = f"cardiffnlp/twitter-roberta-base-{task}"

tokenizer = AutoTokenizer.from_pretrained(MODEL)

# Download label mapping
labels = []
mapping_link = f"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt"
with urllib.request.urlopen(mapping_link) as f:
    html = f.read().decode('utf-8').split("\n")
    csvreader = csv.reader(html, delimiter='\t')
    labels = [row[1] for row in csvreader if len(row) > 1]

# PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL)
model.save_pretrained(MODEL)

for tweet in tweets:
    tweet_text = tweet['full_text']

    text = preprocess(tweet_text)
    encoded_input = tokenizer(text, return_tensors='pt')
    output = model(**encoded_input)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)

    ranking = np.argsort(scores)
    ranking = ranking[::-1]
    for i in range(scores.shape[0]):
        l = labels[ranking[i]]
        s = scores[ranking[i]]
        print("Texto original:", tweet_text + f"\n{i+1}) {l} {np.round(float(s), 4)}")

    print("------------------------")

"""### ANALISIS DE SENTIMIENTO OPEN_IA"""

import openai

openai.organization = "org-sksdkxBOVSxNKI4k8b0Tt5kw"
openai.api_key = "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"

def analizar_sentimiento(texto):
    response = openai.Completion.create(
        engine='text-davinci-003',
        prompt="puedes hacer una analisis de sentimientos con cada texto, ademas puedes etiquetarlos con el tipo de sentimiento por ejemplo si es(positivo,negativo y neutro) y si le pudieras dar un score bayesiano" + texto,
        max_tokens=500,
        temperature=0.2,

    )

    sentiment = response.choices[0].text.strip()
    return sentiment
texto1 = "Hoy ha sido un día maravilloso."
sentimiento1 = analizar_sentimiento(texto1)
print(f"Hoy ha sido un día maravilloso.: {sentimiento1}")

texto2 = "Estoy muy triste por las noticias de hoy."
sentimiento2 = analizar_sentimiento(texto2)
print(f"Estoy muy triste por las noticias de hoy: {sentimiento2}")

texto3 = "Este libro es increíble, lo recomiendo totalmente."
sentimiento3 = analizar_sentimiento(texto3)
print(f"Este libro es increíble, lo recomiendo totalmente: {sentimiento3}")

"""# Analisis de Tópicos"""

!pip install tweepy pymongo
!pip install transformers

import numpy as np
import gensim
import nltk
import pymongo
from gensim import corpora
from gensim.models import CoherenceModel
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from pymongo import MongoClient
from pymongo.server_api import ServerApi
from gensim.utils import simple_preprocess
from gensim.corpora.dictionary import Dictionary
from gensim.models.phrases import Phrases, Phraser
import pandas as pd
import re
from pprint import pprint
from gensim import corpora,models
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

uri = "mongodb+srv://mate01:mcxDZa9yU8aUaK2O@cluster0tweet-gp.hkqaqos.mongodb.net/?retryWrites=true&w=majority"

# Create a new client and connect to the server
client = MongoClient(uri, server_api=ServerApi('1'))

# Send a ping to confirm a successful connection
try:
    client.admin.command('ping')
    print("Pinged your deployment. You successfully connected to MongoDB!")
except Exception as e:
    print(e)

# Base de Datos y Colección
mongo_db = 'Preprocessing'
mongo_collection = 'ecuadorTweets'
db = client[mongo_db]

datos = list(db[mongo_collection].find().limit(100))
tweets = [d['clean_text'] for d in datos]

stop_words = stopwords.words('spanish')
stop_words.extend(['rt'])
stop_words.extend(['q'])

id2word = corpora.Dictionary(tweets)

# Create Corpus
texts = tweets

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

# View
print(corpus[:1][0][:30])

print(corpus[:4][:30])

tfidf_model = models.TfidfModel(corpus)
tfidf_corpus = tfidf_model[corpus]

print(tfidf_corpus[:1][0][:30])

lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=15,
                                       random_state=100,
                                       chunksize=100,
                                       passes=10,
                                       per_word_topics=True)

from pprint import pprint

# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

from gensim.models import CoherenceModel

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=tweets, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)

def compute_coherence_values(corpus, dictionary, k, a, b):

    lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                           id2word=dictionary,
                                           num_topics=k,
                                           random_state=100,
                                           chunksize=100,
                                           passes=10,
                                           alpha=a,
                                           eta=b)

    coherence_model_lda = CoherenceModel(model=lda_model, texts=tweets, dictionary=id2word, coherence='c_v')

    return coherence_model_lda.get_coherence()

import os

# Verificar si el directorio "results" existe y crearlo si no existe
if not os.path.exists('./results'):
    os.makedirs('./results')

# Guardar el archivo en el directorio "results"
pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)



import numpy as np
import tqdm

grid = {}
grid['Validation_Set'] = {}

# Topics range
min_topics = 2
max_topics = 31
step_size = 1
topics_range = range(min_topics, max_topics, step_size)

# Alpha parameter
alpha = list(np.arange(0.01, 1, 0.3))
alpha.append('symmetric')
alpha.append('asymmetric')

# Beta parameter
beta = list(np.arange(0.01, 1, 0.3))
beta.append('symmetric')

# Validation sets
num_of_docs = len(corpus)
corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)),
               corpus]

corpus_title = ['100% Corpus']

model_results = {'Validation_Set': [],
                 'Topics': [],
                 'Alpha': [],
                 'Beta': [],
                 'Coherence': []
                }

# Can take a long time to run
if 1 == 1:
    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))

    # iterate through validation corpuses
    for i in range(len(corpus_sets)):
        # iterate through number of topics
        for k in topics_range:
            # iterate through alpha values
            for a in alpha:
                # iterare through beta values
                for b in beta:
                    # get the coherence score for the given parameters
                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word,
                                                  k=k, a=a, b=b)
                    # Save the model results
                    model_results['Validation_Set'].append(corpus_title)
                    model_results['Topics'].append(k)
                    model_results['Alpha'].append(a)
                    model_results['Beta'].append(b)
                    model_results['Coherence'].append(cv)

                    pbar.update(1)
    pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)
    pbar.close()

from gensim.models import CoherenceModel
coherence = []
for k in range(2,31):
    print('Round: '+str(k))
    Lda = gensim.models.ldamodel.LdaModel
    ldamodel = Lda(corpus, num_topics=k, id2word = id2word, passes=20, iterations=20, chunksize = 25,random_state=10, eval_every = None)

    cm = gensim.models.coherencemodel.CoherenceModel(model=ldamodel, texts= tweets,dictionary= id2word, coherence='c_v')

    coherence.append((k,cm.get_coherence()))

listacv=[]
for i in range(0,29):
  listacv.append(coherence[i][1])
  print(listacv)
  print(len(listacv))

import matplotlib.pyplot as plt
x= range(2,31)
plt.plot(x, listacv)
plt.xlabel("Num Topics")
plt.ylabel("Coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show

num_topics = 6

lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=num_topics,
                                           random_state=100,
                                           chunksize=100,
                                           passes=10,
                                           alpha=0.01,
                                           eta=0.01)

!pip install pyLDAvis

pip install pandas==1.5.3 numpy==1.22.4

import pyLDAvis.gensim_models as gensimvis
import pickle
import pyLDAvis

# Visualize the topics
pyLDAvis.enable_notebook()

LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))

# # this is a bit time consuming - make the if statement True
# # if you want to execute visualization prep yourself
if 1 == 1:
    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)
    with open(LDAvis_data_filepath, 'wb') as f:
        pickle.dump(LDAvis_prepared, f)

# load the pre-prepared pyLDAvis data from disk
with open(LDAvis_data_filepath, 'rb') as f:
    LDAvis_prepared = pickle.load(f)

pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')

LDAvis_prepared