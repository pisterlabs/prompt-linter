# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/08_function_callbacks.ipynb.

# %% auto 0
__all__ = ['AsyncLLMCallback', 'LLMEventType', 'AsyncFunctionalStyleChatCompletionHandler']

# %% ../nbs/08_function_callbacks.ipynb 1
from datetime import datetime
from typing import Any, Dict, List, Optional, Union, Callable, Awaitable
from uuid import UUID
from langchain.callbacks.base import AsyncCallbackHandler
from langchain.schema.messages import BaseMessage
from langchain.schema.output import ChatGenerationChunk, GenerationChunk, LLMResult
from enum import Enum
import asyncio
from traceback import format_exception
from .core import OPENAI_API_KEY
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

# %% ../nbs/08_function_callbacks.ipynb 2
class LLMEventType(Enum):
    LLM_START = "LLM_START"
    LLM_TOKEN = "TOKEN"
    LLM_ERROR = "LLM_ERROR"
    LLM_END = "LLM_END"


AsyncLLMCallback = Callable[[LLMEventType, datetime, str], Awaitable[None]]

# %% ../nbs/08_function_callbacks.ipynb 3
class AsyncFunctionalStyleChatCompletionHandler(AsyncCallbackHandler):
    def __init__(self, callback: AsyncLLMCallback) -> None:
        super().__init__()
        self.llm_callback = callback
    
    async def on_llm_start(self,
                           serialized: Dict[str, Any],
                           prompts: List[str],
                           *,
                           run_id: UUID,
                           parent_run_id: UUID | None = None,
                           tags: List[str] | None = None,
                           metadata: Dict[str, Any] | None = None,
                           **kwargs: Any) -> None:
        assert len(prompts) == 1, "This agent structure works with 1 query each time"
        time = datetime.now()
        await asyncio.gather(*[
            self.llm_callback(
                LLMEventType.LLM_START,
                time,
                prompt
            )
            for prompt in prompts
        ])
    
    async def on_chat_model_start(self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], *, run_id: UUID, parent_run_id: UUID | None = None, tags: List[str] | None = None, metadata: Dict[str, Any] | None = None, **kwargs: Any) -> Any:
        prompts = [
            "\n\n".join([
                f"{message.type}: {message.content}"
                for message in thread
            ])
            for thread in messages
        ]
        await self.on_llm_start(serialized, prompts, run_id=run_id, parent_run_id=parent_run_id, tags=tags, metadata=metadata, **kwargs)
    
    async def on_llm_new_token(self, token: str, *, chunk: GenerationChunk | ChatGenerationChunk | None = None, run_id: UUID, parent_run_id: UUID | None = None, tags: List[str] | None = None, **kwargs: Any) -> None:
        await self.llm_callback(LLMEventType.LLM_TOKEN, datetime.now(), token)
    
    async def on_llm_end(self, response: LLMResult, *, run_id: UUID, parent_run_id: UUID | None = None, tags: List[str] | None = None, **kwargs: Any) -> None:
        assert len(response.generations) == 1, "This agent implementation works with 1 generated response"
        assert len(response.generations[0]) == 1, "This agent implementation works with 1 generated response"
        text = response.generations[0][0].text
        await self.llm_callback(
            LLMEventType.LLM_END,
            datetime.now(),
            text,
        )
    
    async def on_llm_error(self, error: BaseException, *, run_id: UUID, parent_run_id: UUID | None = None, tags: List[str] | None = None, **kwargs: Any) -> None:
        await self.llm_callback(
            LLMEventType.LLM_ERROR,
            datetime.now(),
            "".join(format_exception(error)),
        )
        raise error
