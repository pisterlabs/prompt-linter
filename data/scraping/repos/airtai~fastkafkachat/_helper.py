# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/Helper.ipynb.

# %% auto 0
__all__ = ['get_all_links_from_website', 'extract_latest_doc_urls', 'get_service_context', 'zip_index_files', 'unzip_index_files']

# %% ../nbs/Helper.ipynb 1
from pathlib import Path
from typing import *
import logging
from urllib.request import Request, urlopen
from urllib.parse import urlparse, urljoin
from urllib.error import HTTPError
import zipfile
import os
import glob

from bs4 import BeautifulSoup
from langchain.chat_models import ChatOpenAI
from llama_index import (
    LLMPredictor,
    ServiceContext,
)

# %% ../nbs/Helper.ipynb 3
def get_all_links_from_website(start_url: str, visited: Optional[set] = None) -> Set[str]:
    """Get a set of all links (URLs) found on the given website, starting from the given start URL.
    
    Args:
        start_url: The starting URL of the website.
        visited: Optional. A set of URLs that have already been visited. Defaults to an empty set.

    Returns:
        A set of all links found on the website.
    """
    if visited is None:
        visited = set()
    try:
        req = Request(start_url)
        # nosemgrep: python.lang.security.audit.dynamic-urllib-use-detected.dynamic-urllib-use-detected
        html_page = urlopen(req) # nosec B310
        soup = BeautifulSoup(html_page, "lxml")

        base_url = urlparse(start_url).scheme + '://' + urlparse(start_url).hostname #type: ignore

        links = set()
        for link in soup.find_all('a', href=True):
            url = urljoin(base_url, link['href']).split("#")[0].strip("/")
            if urlparse(url).hostname == urlparse(start_url).hostname:
                links.add(url)

        visited.add(start_url)
        for link in links:
            if link not in visited:
                visited |= get_all_links_from_website(link, visited)
                
    except HTTPError as e:
        logging.warning(f'Unable to parse: {e.url}')
    
    return visited

# %% ../nbs/Helper.ipynb 5
def extract_latest_doc_urls(start_url: str, urls: List[str]) -> List[str]:
    """Extract latest documentation URLs from a list of URLs.

    Args:
        start_url: The URL of the documentation homepage.
        urls: A list of documentation URLs to be filtered.

    Returns:
        A new list containing only the latest version of the documentation URLs.
    """
    ret_val = []
    for url in urls:
        parts = url.split(f"{start_url}/docs/")
        if len(parts) == 1:
            ret_val.append(url)
        else:
            identifier = parts[1].split("/")[0]
            if identifier != "next" and not identifier.replace(".", "").isdigit():
                ret_val.append(url)
    return ret_val

# %% ../nbs/Helper.ipynb 10
def get_service_context() -> ServiceContext:
    """Return a service context object initialized with an LLM predictor based on the gpt-3.5-turbo model
    
    Returns:
        A ServiceContext object with an LLMPredictor and a chunk size limit.
    """
    llm_predictor = LLMPredictor(
        llm=ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
    )
    service_context = ServiceContext.from_defaults(
        llm_predictor=llm_predictor, chunk_size_limit=512
    )
    
    return service_context

# %% ../nbs/Helper.ipynb 12
def zip_index_files(data_dir_path: str) -> None:
    """Compresses all JSON index files within a folder into a ZIP archive.

    Args:
        data_dir_path: The path of the folder to be compressed.
    """
    target_path = os.path.join(data_dir_path, 'website_index.zip')
    with zipfile.ZipFile(target_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        file_paths = glob.glob(os.path.join(data_dir_path, '*.json'))
        for file_path in file_paths:
            file_name = os.path.basename(file_path)
            zipf.write(file_path, arcname=file_name)

# %% ../nbs/Helper.ipynb 14
def unzip_index_files(zip_file_path: str) -> None:
    """Decompresses a ZIP file in the same folder.

    Args:
        zip_file_path: The path of the ZIP file to decompress.
    """
    folder_path = os.path.dirname(zip_file_path)
    with zipfile.ZipFile(zip_file_path, 'r') as zipf:
        zipf.extractall(folder_path)
