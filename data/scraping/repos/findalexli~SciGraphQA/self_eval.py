import argparse
import json
import os

import openai
import tqdm
import time
from datasets import load_dataset
from dotenv import load_dotenv
from tqdm import tqdm
load_dotenv()
first_1000 = load_dataset('alexshengzhili/SciCapInstructed-graph-only-qa', split='1_percent_as_validation[1000:]')
openai.api_key = os.getenv('OPENAI_API_KEY')
system_message = """
You are a helpful and precise assistant for checking the quality of the answer.
You are given the graph's caption, the context of the graph, the abstract, tthe title

And then you are given the question,  the answer generated by the model. Please
think about how helpful the model answer is to the user and rate the model answer on a scale of 0 to 10, 
where 0 is not helpful at all and 10 is very helpful. Just return the floating number between 0 and 10.
"""
# We use a human/AI template to organize the context as a multi-turn conversation.
# <image> denotes an image placehold.


def get_input(example):
    question = example['q_a_pairs'][0][0]

    prompts = [
    f'''The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
    Human: <image>
    Human: {question}.
    AI: ''']
    image_root_folder = '/home/ubuntu/imgs/train/'
    image_filepath = example['image_file']
    return prompts, [image_root_folder + image_filepath,]
def construct_input_string(index):
    content = dict()
    cur_example = first_1000[index]
    content['title'] = cur_example['title']
    content['abstract'] = cur_example['abstract']
    content['caption'] = cur_example['caption']
    content['Question to the model'] = cur_example['q_a_pairs'][0][0]
    content['Candidate model answer'] = cur_example['q_a_pairs'][0][1]
    return json.dumps(content)


def get_openai_response(content_string):
    openai_response = openai.ChatCompletion.create(
                    model='gpt-4-0613',
                    messages=[{
                        'role': 'system',
                        'content': system_message
                    }, {
                        'role': 'user',
                        'content': content_string
                    }],
                    temperature=0.2,  # TODO: figure out which temperature is best for evaluation
                    max_tokens=500,
                )['choices'][0]['message']['content']
    return openai_response
if __name__ == '__main__':
    openai_responses = []
    error_indices = []
    for i in tqdm(range(len(first_1000))):
        try:
            content_string = construct_input_string(i)
            openai_response = get_openai_response(content_string)
            print(openai_response)
            openai_responses.append(openai_response)
            with open("openai_responses_1k.json", "a") as json_file:
                json_file.write(json.dumps(openai_response) + "\n")
            time.sleep(7)
        except Exception as e:
            print(f'error at index {i}')
            print(e)
            error_indices.append((i, e))
            time.sleep(7)

    # save the responses to a pickle file
    import pickle
    with open('openai_responses_self_eval_first_1k.pkl', 'wb') as f:
        pickle.dump(openai_responses, f)

    # save the responses to a pickle file
    import pickle
    with open('openai_responses_self_eval_error_index_first_1k.pkl', 'wb') as f:
        pickle.dump(error_indices, f)